{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64072154",
   "metadata": {},
   "source": [
    "# Workshop 2: Prompt Engineering\n",
    "\n",
    "In this workshop, we explore prompt engineering techniques that form the foundation of working with LLMs. We start with fundamental prompting strategies (zero-shot, few-shot, chain-of-thought), then discover how LLMs can do much more than generate text -- they can take on roles, output structured data, generate and execute code, and even debug their own mistakes.\n",
    "\n",
    "**Duration:** ~35-40 minutes of guided walkthrough + practice exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfae7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display utilities for clean notebook formatting\n\n",
    "from utils.display import output_box, compare_table, llm_response, heading, separator\n\n",
    "\n\n",
    "from openai import OpenAI\n\n",
    "\n\n",
    "openai_client = OpenAI()\n\n",
    "\n\n",
    "\n\n",
    "def generate(prompt):\n\n",
    "    \"\"\"Generate a response from the LLM. Uses temperature=0 for deterministic output.\"\"\"\n\n",
    "    response = openai_client.chat.completions.create(\n\n",
    "        model=\"gpt-4o-mini\",\n\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful and unbiased assistant.\"},\n\n",
    "                  {\"role\": \"user\", \"content\": prompt}],\n\n",
    "        temperature=0  # Deterministic output for reproducible examples\n\n",
    "    )\n\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3419ec63",
   "metadata": {},
   "source": [
    "## Part 1: Fundamental Prompting Techniques\n",
    "\n",
    "In this section, we'll explore the core prompting strategies that form the foundation of working with LLMs. We'll use a consistent task across all examples so you can clearly see how each technique affects the model's behavior.\n",
    "\n",
    "### 1. Zero-Shot Prompting\n",
    "\n",
    "**What is it?** Zero-shot prompting means asking the model to perform a task without any examples. You're relying entirely on the model's pre-trained knowledge.\n",
    "\n",
    "**When to use it:**\n",
    "- Simple, well-defined tasks\n",
    "- When you want quick results without setup\n",
    "\n",
    "**Watch what happens:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede500c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot: No examples given\n\n",
    "prompt = \"Create an acronym from 'artificial neural network' using the last letter of each word.\"\n\n",
    "zero_shot_output = generate(prompt)\n\n",
    "\n\n",
    "print(f\"Prompt: {prompt}\")\n\n",
    "print(f\"\\nOutput: {zero_shot_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326361ca",
   "metadata": {},
   "source": [
    "### 2. One-Shot Prompting\n",
    "\n",
    "**What is it?** One-shot prompting provides an example before asking the model to perform the task. This helps the model understand the pattern you want.\n",
    "\n",
    "**When to use it:**\n",
    "- Tasks with specific formats\n",
    "- When zero-shot doesn't work well\n",
    "- Teaching a simple pattern quickly\n",
    "\n",
    "**Watch what happens:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49184dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-shot: One example provided\n\n",
    "prompt = \"\"\"Create an acronym using the last letter of each word:\n\n",
    "\n\n",
    "\"big red car\" -> GDR\n\n",
    "\"artificial neural network\" ->\"\"\"\n\n",
    "\n\n",
    "one_shot_output = generate(prompt)\n\n",
    "\n\n",
    "print(f\"Prompt: {prompt}\")\n\n",
    "print(f\"\\nOutput: {one_shot_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0cb9be",
   "metadata": {},
   "source": [
    "### 3. Few-Shot Prompting\n",
    "\n",
    "**What is it?** Few-shot prompting provides multiple examples (typically 2-5) to establish a clear pattern. The model learns from these examples and applies the pattern to new inputs.\n",
    "\n",
    "**When to use it:**\n",
    "- Complex patterns that need reinforcement\n",
    "- Tasks requiring specific formatting\n",
    "- When consistency is critical\n",
    "\n",
    "**Multiple examples = stronger pattern:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472031e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot: Multiple examples provided\n\n",
    "prompt = \"\"\"Create an acronym using the last letter of each word:\n\n",
    "\n\n",
    "\"big red car\" -> GDR\n\n",
    "\"machine learning system\" -> EGM\n\n",
    "\"deep learning model\" -> PGL\n\n",
    "\"artificial neural network\" ->\"\"\"\n\n",
    "\n\n",
    "few_shot_output = generate(prompt)\n\n",
    "\n\n",
    "print(f\"Prompt: {prompt}\")\n\n",
    "print(f\"\\nOutput: {few_shot_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae2ee26",
   "metadata": {},
   "source": [
    "### 4. Zero-Shot Chain-of-Thought (CoT)\n",
    "\n",
    "**What is it?** By adding \"Think step-by-step\" or similar phrases, we trigger the model to show its reasoning process. This often leads to more accurate results, especially for tasks requiring logic or calculation.\n",
    "\n",
    "**When to use it:**\n",
    "- Tasks requiring the model to reason correctly\n",
    "- When you need to verify the model's thinking\n",
    "\n",
    "**Magic phrase: \"Think step-by-step\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5557fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot CoT: No examples, but asking for reasoning\n\n",
    "prompt = \"Create an acronym from 'artificial neural network' using the last letter of each word. Think step-by-step.\"\n\n",
    "\n\n",
    "zero_shot_cot_output_1 = generate(prompt)\n\n",
    "\n\n",
    "print(f\"Prompt: {prompt}\")\n\n",
    "print(f\"\\nOutput: {zero_shot_cot_output_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f5284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another CoT trigger phrase\n\n",
    "prompt = \"Create an acronym from 'artificial neural network' using the last letter of each word. Before answering, first explain your work!\"\n\n",
    "\n\n",
    "zero_shot_cot_output_2 = generate(prompt)\n\n",
    "\n\n",
    "print(f\"Prompt: {prompt}\")\n\n",
    "print(f\"\\nOutput: {zero_shot_cot_output_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040b5482",
   "metadata": {},
   "source": [
    "### 5. Few-Shot Chain-of-Thought (CoT)\n",
    "\n",
    "**What is it?** A powerful combination -- providing examples that demonstrate step-by-step reasoning. The model learns both the pattern AND the reasoning process.\n",
    "\n",
    "**When to use it:**\n",
    "- Complex tasks requiring specific reasoning\n",
    "- Teaching the model a particular problem-solving approach\n",
    "- Maximum accuracy is needed\n",
    "\n",
    "**Examples + Reasoning = Very good results:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d08ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot CoT: Examples with step-by-step reasoning\n\n",
    "prompt = \"\"\"Create an acronym using the last letter of each word. Think step-by-step.\n\n",
    "\n\n",
    "Here are some examples:\n\n",
    "\n\n",
    "Example 1: \n\n",
    "Phrase: \"big red car\"\n\n",
    "- Words: big, red, car\n\n",
    "- Last letters: g, d, r\n\n",
    "- Result: GDR\n\n",
    "\n\n",
    "Example 2: \n\n",
    "Phrase: \"machine learning system\"\n\n",
    "- Words: machine, learning, system\n\n",
    "- Last letters: e, g, m\n\n",
    "- Result: EGM\n\n",
    "\n\n",
    "Now solve: \n\n",
    "Phrase: \"artificial neural network\"\n\n",
    "\"\"\"\n\n",
    "\n\n",
    "few_shot_cot_output = generate(prompt)\n\n",
    "\n\n",
    "print(f\"Prompt: {prompt}\")\n\n",
    "print(f\"\\nOutput: {few_shot_cot_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5145ef9e",
   "metadata": {},
   "source": [
    "### Comparison\n",
    "\n",
    "Let's compare all techniques side by side to see how each approach affects the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388c9a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all techniques side by side\n\n",
    "techniques = [\n\n",
    "    (\"Zero-shot\", zero_shot_output),\n\n",
    "    (\"One-shot\", one_shot_output),\n\n",
    "    (\"Few-shot\", few_shot_output),\n\n",
    "    (\"Zero-shot CoT (v1)\", zero_shot_cot_output_1.split('\\n')[-1] if '\\n' in zero_shot_cot_output_1 else zero_shot_cot_output_1),\n\n",
    "    (\"Zero-shot CoT (v2)\", zero_shot_cot_output_2.split('\\n')[-1] if '\\n' in zero_shot_cot_output_2 else zero_shot_cot_output_2),\n\n",
    "    (\"Few-shot CoT\", few_shot_cot_output.split('\\n')[-1] if '\\n' in few_shot_cot_output else few_shot_cot_output),\n\n",
    "]\n\n",
    "\n\n",
    "compare_table(techniques, headers=(\"Technique\", \"Output\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4618e32",
   "metadata": {},
   "source": [
    "> **Key Insight:** Notice how zero-shot and few-shot without CoT get the wrong answer, while chain-of-thought reasoning produces the correct result (LLK). Forcing the model to \"show its work\" dramatically improves accuracy on reasoning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3c166d",
   "metadata": {},
   "source": [
    "## Part 2: Beyond Text Generation\n",
    "\n",
    "So far, we've seen that LLMs are fundamentally:\n",
    "**Text Input -> LLM -> Text Output**\n",
    "\n",
    "But what if we could do more? Let me introduce a series of \"what if\" questions. Each question will unlock a new capability, gradually revealing how simple text generators can evolve into more complex things."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a0c2c3",
   "metadata": {},
   "source": [
    "### What if we want to change the LLM's behavior or personality?\n",
    "\n",
    "Right now, every time we use an LLM, it responds with its default personality and style. But what if we could give it a specific role or personality? What if we could make it behave like a Python expert, or a data scientist, or even a pirate?\n",
    "\n",
    "**Enter: System Prompts!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37c9862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function that accepts a system prompt\n\n",
    "def generate_with_system(system_prompt, user_prompt, temperature=0):\n\n",
    "    \"\"\"Generate a response with a custom system prompt.\"\"\"\n\n",
    "    response = openai_client.chat.completions.create(\n\n",
    "        model=\"gpt-4o-mini\",\n\n",
    "        messages=[\n\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n\n",
    "        ],\n\n",
    "        temperature=temperature\n\n",
    "    )\n\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdbae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different roles with the same question\n\n",
    "roles = {\n\n",
    "    \"Pirate\": \"You are a pirate. You always respond in pirate speak.\",\n\n",
    "    \"Shakespeare\": \"You are Shakespeare. Respond in Elizabethan English with poetic flair.\",\n\n",
    "    \"Data Scientist\": \"You are a data scientist. Use technical terms and be precise.\"\n\n",
    "}\n\n",
    "\n\n",
    "user_prompt = \"Explain what Large Language Models are in less than 50 words.\"\n\n",
    "\n\n",
    "print(\"Same question, different roles:\\n\")\n\n",
    "for role_name, system_prompt in roles.items():\n\n",
    "    response = generate_with_system(system_prompt, user_prompt)\n\n",
    "    output_box(response, label=role_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c176248",
   "metadata": {},
   "source": [
    "> **Key Insight:** System prompts are like giving the LLM a job description! They define how it should behave, what expertise it should have, and what constraints it should follow. This is the first step toward making LLMs useful for specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5270ab",
   "metadata": {},
   "source": [
    "### What if we want deterministic, structured output?\n",
    "\n",
    "You might have noticed that LLMs sometimes give different answers to the same question, more often when the temperature is high. This randomness can be a problem when we need consistent, reliable outputs. What if we could control not just WHAT the LLM says, but HOW it structures its response?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f2a9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n\n",
    "\n\n",
    "# Example 1: JSON output via prompting\n\n",
    "json_prompt = \"\"\"Extract information from this text and return ONLY valid JSON:\n\n",
    "\n\n",
    "\"Alice and Bob are meeting for lunch tomorrow at 2 PM at the Italian restaurant downtown.\"\n\n",
    "\n\n",
    "Format:\n\n",
    "{\n\n",
    "  \"participants\": [...],\n\n",
    "  \"time\": \"...\",\n\n",
    "  \"location\": \"...\"\n\n",
    "}\n\n",
    "\"\"\"\n\n",
    "\n\n",
    "response = generate(json_prompt)\n\n",
    "print(\"JSON Output:\")\n\n",
    "print(response)\n\n",
    "\n\n",
    "# Parse it to verify it's valid JSON\n\n",
    "try:\n\n",
    "    data = json.loads(response)\n\n",
    "    print(f\"\\nValid JSON! Parsed data: {data}\")\n\n",
    "except json.JSONDecodeError as e:\n\n",
    "    print(f\"\\nInvalid JSON: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db83647c",
   "metadata": {},
   "source": [
    "OK, that worked! But this can be brittle. There is a better way.\n",
    "\n",
    "#### Recommended method for generating structured output\n",
    "\n",
    "While prompting to output specific format may give the desired output a lot of the time, most LLMs these days have the \"Structured Output\" feature. It allows us to provide an output format via JSON Schema or Pydantic (a Python data validation library that uses type annotations to define schemas). These methods guarantee that the output will conform to your specified structure, eliminating parsing errors and invalid formats.\n",
    "\n",
    "See: https://platform.openai.com/docs/guides/structured-outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f670e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Structured output with Pydantic (OpenAI's structured outputs)\n\n",
    "from pydantic import BaseModel\n\n",
    "from openai import OpenAI\n\n",
    "\n\n",
    "client = OpenAI()\n\n",
    "\n\n",
    "\n\n",
    "class CalendarEvent(BaseModel):\n\n",
    "    name: str\n\n",
    "    date: str\n\n",
    "    participants: list[str]\n\n",
    "\n\n",
    "\n\n",
    "completion = client.chat.completions.parse(\n\n",
    "    model=\"gpt-4o-mini\",\n\n",
    "    messages=[\n\n",
    "        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n\n",
    "        {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n\n",
    "    ],\n\n",
    "    response_format=CalendarEvent,\n\n",
    ")\n\n",
    "\n\n",
    "event = completion.choices[0].message.parsed\n\n",
    "\n\n",
    "print(\"Structured Output with Pydantic:\")\n\n",
    "print(f\"Event Name: {event.name}\")\n\n",
    "print(f\"Event Date: {event.date}\")\n\n",
    "print(f\"Participants: {event.participants}\")\n\n",
    "print(\"\\nGuaranteed valid structure!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ef212d",
   "metadata": {},
   "source": [
    "> **Key Insight:** We can force LLMs to output in specific formats like JSON or use Pydantic schemas to guarantee structure. This makes their outputs predictable and parseable by code!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78d3702",
   "metadata": {},
   "source": [
    "### Since LLMs can output text, can we actually output code?\n",
    "\n",
    "We've seen LLMs generate formatted text and JSON. But code is just text too, right? What if we asked an LLM to write actual, executable code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d297bb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask LLM to generate pandas code\n\n",
    "prompt = \"\"\"Generate Python code using pandas to:\n\n",
    "1. Create a DataFrame with sales data (product, price, quantity)\n\n",
    "2. Calculate total revenue (price * quantity)\n\n",
    "3. Find the product with highest revenue\n\n",
    "\n\n",
    "Output ONLY executable Python code.\"\"\"\n\n",
    "\n\n",
    "code_output = generate(prompt)\n\n",
    "print(\"Generated Code:\")\n\n",
    "separator()\n\n",
    "print(code_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2fb5cb",
   "metadata": {},
   "source": [
    "### If we can output code, what if we actually RUN that code?\n",
    "\n",
    "The LLM just generated code as text. But it's valid Python code... What if we could execute it programmatically? What if our program could take the LLM's code and actually run it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0c5acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract just the code (remove markdown formatting if any)\n\n",
    "if \"```python\" in code_output:\n\n",
    "    actual_code = code_output.split(\"```python\")[1].split(\"```\")[0]\n\n",
    "else:\n\n",
    "    actual_code = code_output\n\n",
    "\n\n",
    "print(\"Executing the LLM-generated code...\")\n\n",
    "separator()\n\n",
    "\n\n",
    "# Use exec() to run the code\n\n",
    "import pandas as pd\n\n",
    "exec(actual_code)\n\n",
    "\n\n",
    "print(\"\\nThe LLM's code ran successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d543da",
   "metadata": {},
   "source": [
    "### What if we feed the output back to the LLM?\n",
    "\n",
    "So we can generate code and run it. But what happens after? The code produces output... What if we could capture that output and feed it back to the LLM? What if the LLM could see the results of its own code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed021e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n\n",
    "import sys\n\n",
    "\n\n",
    "\n\n",
    "def capture_output(code):\n\n",
    "    \"\"\"Execute code and capture its printed output.\"\"\"\n\n",
    "    old_stdout = sys.stdout\n\n",
    "    sys.stdout = captured_output = io.StringIO()\n\n",
    "\n\n",
    "    try:\n\n",
    "        exec(code, {'pd': pd})\n\n",
    "        output = captured_output.getvalue()\n\n",
    "    finally:\n\n",
    "        sys.stdout = old_stdout\n\n",
    "\n\n",
    "    return output\n\n",
    "\n\n",
    "\n\n",
    "# Step 1: Generate code\n\n",
    "prompt1 = \"Generate pandas code to create a DataFrame with 5 random numbers and calculate their mean. Print the DataFrame and the mean.\"\n\n",
    "code1 = generate(prompt1)\n\n",
    "\n\n",
    "# Clean the code\n\n",
    "if \"```python\" in code1:\n\n",
    "    code1 = code1.split(\"```python\")[1].split(\"```\")[0]\n\n",
    "\n\n",
    "print(\"Generated Code:\")\n\n",
    "print(code1)\n\n",
    "separator()\n\n",
    "\n\n",
    "# Step 2: Run and capture output\n\n",
    "output = capture_output(code1)\n\n",
    "print(\"Captured Output:\")\n\n",
    "print(output)\n\n",
    "separator()\n\n",
    "\n\n",
    "# Step 3: Feed output back to LLM\n\n",
    "prompt2 = f\"\"\"Based on this output from a pandas operation:\n\n",
    "\n\n",
    "{output}\n\n",
    "\n\n",
    "What is the mean value? Is it above or below 0.5?\"\"\"\n\n",
    "\n\n",
    "analysis = generate(prompt2)\n\n",
    "print(\"LLM Analysis of its own output:\")\n\n",
    "llm_response(analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e57da5",
   "metadata": {},
   "source": [
    "> **Key Insight:** We've closed the loop! The LLM can see and analyze the results of its own code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb0dec8",
   "metadata": {},
   "source": [
    "### What if the LLM's code has errors? Can it fix them?\n",
    "\n",
    "Sometimes the code the LLM generates has errors. But if we can capture those errors and feed them back... What if the LLM could debug its own code? What if it could learn from its mistakes and try again?\n",
    "\n",
    "Let's intentionally cause an error and see if the LLM can fix it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "vavo27idsqh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1:\n",
      "\n",
      "# Calculate the average grade\n",
      "average_grade = sum(student_grades) / len(student_grades) if student_grades else 0\n",
      "\n",
      "# Print the average grade\n",
      "print(f\"Average Grade: {average_grade:.2f}\")\n",
      "\n",
      "# Print each student with their index and ID\n",
      "for index, student_id in enumerate(student_ids):\n",
      "    print(f\"Index: {index}, Student ID: {student_id}, Grade: {student_grades[index]}\")\n",
      "\n",
      "\n",
      "\u274c Error: name 'student_grades' is not defined\n",
      "\ud83d\udd04 Asking LLM to fix it...\n",
      "\n",
      "Attempt 2:\n",
      "\n",
      "# Available context\n",
      "data = {'grades': [85, 92, 78, 95, 88], 'students': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve']}\n",
      "\n",
      "# Extract grades and student IDs\n",
      "student_grades = data['grades']\n",
      "student_ids = data['students']\n",
      "\n",
      "# Calculate the average grade\n",
      "average_grade = sum(student_grades) / len(student_grades) if student_grades else 0\n",
      "\n",
      "# Print the average grade\n",
      "print(f\"Average Grade: {average_grade:.2f}\")\n",
      "\n",
      "# Print each student with their index and ID\n",
      "for index, student_id in enumerate(student_ids):\n",
      "    print(f\"Index: {index}, Student ID: {student_id}, Grade: {student_grades[index]}\")\n",
      "\n",
      "\n",
      "Average Grade: 87.60\n",
      "Index: 0, Student ID: Alice, Grade: 85\n",
      "Index: 1, Student ID: Bob, Grade: 92\n",
      "Index: 2, Student ID: Charlie, Grade: 78\n",
      "Index: 3, Student ID: Diana, Grade: 95\n",
      "Index: 4, Student ID: Eve, Grade: 88\n",
      "\u2705 Success!\n"
     ]
    }
   ],
   "source": [
    "def run_code_with_error_fixing(task, max_attempts=3):\n",
    "    \"\"\"Run LLM-generated code, automatically fixing errors\"\"\"\n",
    "    \n",
    "    grades = [85, 92, 78, 95, 88]\n",
    "    students = ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve']\n",
    "    \n",
    "    context = {'grades': grades, 'students': students}\n",
    "    for attempt in range(max_attempts):\n",
    "        if attempt == 0:\n",
    "            # First attempt - tell LLM to use wrong variable names\n",
    "            prompt = f\"\"\"Write Python code to: {task}\n",
    "You have access to the following variables in memory:\n",
    "- student_grades (list of numbers)\n",
    "- student_ids (list of IDs)\n",
    "\n",
    "Do not create these variables, they already exist!!\n",
    "\"\"\"\n",
    "        else:\n",
    "            # Retry with actual variable names\n",
    "            prompt = f\"\"\"Your previous code had an error:\n",
    "Error: {error_msg}\n",
    "Failed code:\n",
    "{code}\n",
    "\n",
    "Task: {task}\n",
    "\n",
    "Available Context:\n",
    "{context} \n",
    "\n",
    "Fix any errors in the code:\"\"\"\n",
    "        \n",
    "        # Generate code\n",
    "        code = generate(prompt)\n",
    "        if \"```python\" in code:\n",
    "            code = code.split(\"```python\")[1].split(\"```\")[0]\n",
    "        \n",
    "        print(f\"Attempt {attempt + 1}:\")\n",
    "        print(code)\n",
    "        print()\n",
    "        \n",
    "        # Try to execute\n",
    "        try:\n",
    "            \n",
    "            exec(code, {'__builtins__': __builtins__}, context)\n",
    "            \n",
    "            print(\"\u2705 Success!\")\n",
    "            return code\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            print(f\"\u274c Error: {error_msg}\")\n",
    "            if attempt < max_attempts - 1:\n",
    "                print(\"\ud83d\udd04 Asking LLM to fix it...\\n\")\n",
    "    \n",
    "    print(\"\u274c Max attempts reached\")\n",
    "    return None\n",
    "\n",
    "# Run the test\n",
    "result = run_code_with_error_fixing(\n",
    "    \"Calculate the average grade and print each student with their index\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1147da5f",
   "metadata": {},
   "source": [
    "### Putting It All Together\n",
    "\n",
    "To review:\n",
    "\n",
    "1. **System Prompts** -- We can control the LLM's behavior and expertise\n",
    "2. **Format Control** -- We can make outputs predictable and parseable\n",
    "3. **Code Generation** -- LLMs can write executable code\n",
    "4. **Code Execution** -- We can run that code programmatically\n",
    "5. **Output Capture** -- We can feed results back to the LLM\n",
    "6. **Error Handling** -- The LLM can debug and fix its own mistakes\n",
    "\n",
    "When you combine all these capabilities, something magical happens...\n",
    "\n",
    "We've built something beyond simple text generation, that can:\n",
    "- Understand requests (via prompts)\n",
    "- Generate solutions (as code)\n",
    "- Execute actions (run the code)\n",
    "- Observe results (capture output)\n",
    "- Learn from mistakes (error handling)\n",
    "- Maintain context (remember previous operations)\n",
    "\n",
    "Let's see everything in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12140ed1-442f-4feb-9c7f-8066e91b7e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udcca App initialized with sales data:\n",
      "     product  price  quantity     category\n",
      "0     Laptop   1200         5  Electronics\n",
      "1      Mouse     25        50  Accessories\n",
      "2   Keyboard     75        20  Accessories\n",
      "3    Monitor    300        10  Electronics\n",
      "4  USB Cable     10       100  Accessories\n",
      "\n",
      "============================================================\n",
      "\ud83d\udc64 User: What's the total revenue for each category?\n",
      "\ud83e\udd16 LLM generating code...\n",
      "\u274c Error: name 'df' is not defined\n",
      "\ud83d\udd04 Attempting to fix...\n",
      "\ud83d\udcbb Generated code:\n",
      "--------------------------------------------------\n",
      "\n",
      "df = my_df\n",
      "df['revenue'] = df['price'] * df['quantity']\n",
      "total_revenue_by_category = df.groupby('category')['revenue'].sum().reset_index()\n",
      "print(total_revenue_by_category)\n",
      "\n",
      "--------------------------------------------------\n",
      "\u2705 Execution Result:\n",
      "      category  revenue\n",
      "0  Accessories     3750\n",
      "1  Electronics     9000\n",
      "\n",
      "\n",
      "============================================================\n",
      "\ud83d\udc64 User: Which product has the highest revenue?\n",
      "\ud83e\udd16 LLM generating code...\n",
      "\u274c Error: name 'df' is not defined\n",
      "\ud83d\udd04 Attempting to fix...\n",
      "\ud83d\udcbb Generated code:\n",
      "--------------------------------------------------\n",
      "\n",
      "df = my_df\n",
      "highest_revenue_product = df.groupby('product')['revenue'].sum().idxmax()\n",
      "print(highest_revenue_product)\n",
      "\n",
      "--------------------------------------------------\n",
      "\u2705 Execution Result:\n",
      "Laptop\n",
      "\n",
      "\n",
      "============================================================\n",
      "\ud83d\udc64 User: Show me statistics about the prices\n",
      "\ud83e\udd16 LLM generating code...\n",
      "\u274c Error: name 'prices' is not defined\n",
      "\ud83d\udd04 Attempting to fix...\n",
      "\ud83d\udcbb Generated code:\n",
      "--------------------------------------------------\n",
      "\n",
      "# Accessing the 'price' column from the DataFrame 'my_df'\n",
      "statistics = my_df['price'].describe()\n",
      "print(statistics)\n",
      "\n",
      "--------------------------------------------------\n",
      "\u2705 Execution Result:\n",
      "count       5.000000\n",
      "mean      322.000000\n",
      "std       504.462585\n",
      "min        10.000000\n",
      "25%        25.000000\n",
      "50%        75.000000\n",
      "75%       300.000000\n",
      "max      1200.000000\n",
      "Name: price, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import io\n",
    "\n",
    "class App:\n",
    "    \"\"\"A simple app that can analyze data using pandas.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the app with sample sales data and system prompt.\"\"\"\n",
    "        \n",
    "        # Define how the app should behave\n",
    "        self.system_prompt = \"\"\"You are a pandas expert. \n",
    "        Generate ONLY executable Python code.\n",
    "        Do not create sample data. Assume that the variables required are available.\n",
    "        Always print the results.\"\"\"\n",
    "        \n",
    "        # Create sample sales data\n",
    "        self.df = pd.DataFrame({\n",
    "            'product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'USB Cable'],\n",
    "            'price': [1200, 25, 75, 300, 10],\n",
    "            'quantity': [5, 50, 20, 10, 100],\n",
    "            'category': ['Electronics', 'Accessories', 'Accessories', \n",
    "                        'Electronics', 'Accessories']\n",
    "        })\n",
    "        \n",
    "        # Display the initialized data\n",
    "        print(\"\ud83d\udcca App initialized with sales data:\")\n",
    "        print(self.df)\n",
    "        print()\n",
    "    \n",
    "    def execute_request(self, user_request):\n",
    "        \"\"\"\n",
    "        Process a user request by generating and executing pandas code.\n",
    "        \n",
    "        Args:\n",
    "            user_request: Natural language request for data analysis\n",
    "            \n",
    "        Returns:\n",
    "            The output from executing the generated code, or None if failed\n",
    "        \"\"\"\n",
    "\n",
    "        context = {'my_df': self.df, 'pd': pd}\n",
    "        print(f\"\ud83d\udc64 User: {user_request}\")\n",
    "        print(f\"\ud83e\udd16 LLM generating code...\")\n",
    "        \n",
    "        # Generate initial code from the request\n",
    "        code = self._generate_code(user_request)\n",
    "        \n",
    "        # Try to execute the code (with one retry on failure)\n",
    "        MAX_ATTEMPTS = 2\n",
    "        \n",
    "        for attempt in range(MAX_ATTEMPTS):\n",
    "            try:\n",
    "                # Execute the code and capture output\n",
    "                output = self._execute_code(code, context)\n",
    "                \n",
    "                # Display successful results\n",
    "                self._display_success(code, output)\n",
    "                return output\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Handle execution errors\n",
    "                if attempt == 0:\n",
    "                    # First attempt failed - try to fix it\n",
    "                    print(f\"\u274c Error: {e}\")\n",
    "                    print(\"\ud83d\udd04 Attempting to fix...\")\n",
    "                    \n",
    "                    # Generate fixed code with error context\n",
    "                    code = self._generate_fix(user_request, code, e, context)\n",
    "                else:\n",
    "                    # Second attempt also failed\n",
    "                    print(f\"\u274c Could not complete request: {e}\")\n",
    "                    return None\n",
    "    \n",
    "    def _generate_code(self, prompt):\n",
    "        \"\"\"Extract Python code from LLM response.\"\"\"\n",
    "        code = generate_with_system(self.system_prompt, prompt)\n",
    "        \n",
    "        # Extract code from markdown code blocks if present\n",
    "        if \"```python\" in code:\n",
    "            code = code.split(\"```python\")[1].split(\"```\")[0]\n",
    "        \n",
    "        return code\n",
    "    \n",
    "    def _generate_fix(self, original_request, failed_code, error, context):\n",
    "        \"\"\"Generate fixed code based on the error.\"\"\"\n",
    "        error_prompt = f\"\"\"Previous code failed with: {error}\n",
    "        \n",
    "Task: {original_request}\n",
    "Failed code: {failed_code}\n",
    "\n",
    "Original Context: \n",
    "- Variables available in memory: {context.keys()}  <== You need to use these\n",
    "- DataFrame columns: {list(self.df.columns)}\n",
    "- Data shape: {self.df.shape}\n",
    "- Sample data:\n",
    "{self.df.head()}\n",
    "\n",
    "Generate fixed code:\"\"\"\n",
    "        \n",
    "        return self._generate_code(error_prompt)\n",
    "    \n",
    "    def _execute_code(self, code, context):\n",
    "        \"\"\"Execute Python code and capture its output.\"\"\"\n",
    "        # Redirect stdout to capture print statements\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = captured_output = io.StringIO()\n",
    "        \n",
    "        try:\n",
    "            # Execute code with DataFrame and pandas available\n",
    "            exec(code, context)\n",
    "            \n",
    "            # Get the captured output\n",
    "            output = captured_output.getvalue()\n",
    "            return output\n",
    "            \n",
    "        finally:\n",
    "            # Always restore stdout\n",
    "            sys.stdout = old_stdout\n",
    "    \n",
    "    def _display_success(self, code, output):\n",
    "        \"\"\"Display successful code execution results.\"\"\"\n",
    "        print(f\"\ud83d\udcbb Generated code:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(code)\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"\u2705 Execution Result:\")\n",
    "        print(output)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# DEMONSTRATION\n",
    "# ============================================================\n",
    "\n",
    "def run_demo():\n",
    "    app = App()\n",
    "    \n",
    "    requests = [\n",
    "        \"What's the total revenue for each category?\",\n",
    "        \"Which product has the highest revenue?\",\n",
    "        \"Show me statistics about the prices\"\n",
    "    ]\n",
    "    \n",
    "    # Execute each request\n",
    "    for i, request in enumerate(requests, 1):\n",
    "        if i > 1:\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "        else:\n",
    "            print(\"=\" * 60)\n",
    "        \n",
    "        app.execute_request(request)\n",
    "    \n",
    "\n",
    "run_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1106a6",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "\n",
    "We started with a simple question: \"What if we could do more than just generate text?\"\n",
    "\n",
    "Through a series of discoveries, we found that LLMs can enable not just simple text generation, but more complex applications that can:\n",
    "- Take on specific roles and expertise\n",
    "- Generate structured, parseable outputs\n",
    "- Write and execute code\n",
    "- Learn from their outputs\n",
    "- Fix their own mistakes\n",
    "\n",
    "This is the **paradigm shift** in programming we talked about last week:\n",
    "- **Before**: We write every line of code, for errors, we use try...except, and write explicit handling logic for cases we expected\n",
    "- **Now**: We leverage the semantic natural language understanding capabilities of LLMs, to build highly dynamic applications\n",
    "\n",
    "The app we just built is simple, but it demonstrates the core principle: **LLMs are not just text generators, they're reasoning engines that can be given tools and autonomy.**\n",
    "\n",
    "Next week, we'll take this further and start building our full-featured data-analytics agent that can handle any data analysis task through natural language!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95605281",
   "metadata": {},
   "source": [
    "## Part 3: Practice Exercises\n",
    "\n",
    "Try these exercises to reinforce what we covered today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b623b753",
   "metadata": {},
   "source": [
    "### Exercise 1: Math Problem Solving\n",
    "\n",
    "Create a prompt that reliably solves word problems. Test with this problem:\n",
    "\"If a train travels 120 miles in 2 hours, and then 180 miles in 3 hours, what is its average speed for the entire journey?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "p747buj3xh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find the average speed for the entire journey, we first need to calculate the total distance traveled and the total time taken.\n",
      "\n",
      "1. **Calculate the total distance:**\n",
      "   - The first part of the journey is 120 miles.\n",
      "   - The second part of the journey is 180 miles.\n",
      "   - Total distance = 120 miles + 180 miles = 300 miles.\n",
      "\n",
      "2. **Calculate the total time:**\n",
      "   - The time for the first part of the journey is 2 hours.\n",
      "   - The time for the second part of the journey is 3 hours.\n",
      "   - Total time = 2 hours + 3 hours = 5 hours.\n",
      "\n",
      "3. **Calculate the average speed:**\n",
      "   - Average speed = Total distance / Total time\n",
      "   - Average speed = 300 miles / 5 hours = 60 miles per hour.\n",
      "\n",
      "Therefore, the average speed for the entire journey is **60 miles per hour**.\n"
     ]
    }
   ],
   "source": [
    "problem = \"If a train travels 120 miles in 2 hours, and then 180 miles in 3 hours, what is its average speed for the entire journey?\"\n",
    "\n",
    "# Try different approaches:\n",
    "# 1. Zero-shot\n",
    "# 2. With \"think step-by-step\"\n",
    "# 3. With few-shot examples\n",
    "\n",
    "your_prompt = f\"TODO: Your user prompt here\"\n",
    "solution = generate(your_prompt)\n",
    "print(solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2fce48",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Click to reveal solution</strong></summary>\n",
    "\n",
    "```python\n",
    "problem = \"If a train travels 120 miles in 2 hours, and then 180 miles in 3 hours, what is its average speed for the entire journey?\"\n",
    "\n",
    "# Using Chain-of-Thought for reliable math reasoning\n",
    "your_prompt = f\"{problem} Think step-by-step.\"\n",
    "solution = generate(your_prompt)\n",
    "print(solution)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e9d99e",
   "metadata": {},
   "source": [
    "### Exercise 2: Structured Data Extraction\n",
    "\n",
    "Extract structured information from unstructured food delivery orders using three different approaches.\n",
    "\n",
    "Your task:\n",
    "1. **Text Prompting** -- Write prompts to extract order details as formatted text\n",
    "2. **JSON Prompting** -- Modify prompts to return valid JSON\n",
    "3. **Structured Output** -- Define a Pydantic model and use the [LLM's structured output API](https://platform.openai.com/docs/guides/structured-outputs)\n",
    "\n",
    "For each order, extract:\n",
    "- Items ordered (with quantities)\n",
    "- Special modifications/requests\n",
    "- Drinks\n",
    "- Delivery/pickup preference\n",
    "\n",
    "*Just focus on the sections marked as **TODO** for this exercise*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934780fb-2c7e-4fa4-a741-59068e722aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional, Literal\n",
    "\n",
    "# Sample food orders - all contain similar info in different formats\n",
    "food_orders = [\n",
    "    \"2 pepperoni pizzas large, 1 greek salad, and 3 cokes for delivery to 123 Main St\",\n",
    "    \"One burger medium with no onions, extra cheese, side of fries and a chocolate shake\",\n",
    "    \"3x chicken tacos, 1x beef burrito no beans, 2 large horchatas, extra hot sauce please\",\n",
    "    \"Pad Thai mild spice, Tom Yum soup, 2 spring rolls, and jasmine tea for pickup\",\n",
    "    \"Large coffee with oat milk and 2 sugars, plus a blueberry muffin warmed up\",\n",
    "    \"Family meal: 1 whole roast chicken, mashed potatoes, coleslaw, 4 dinner rolls\"\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# LLM HELPER FUNCTION\n",
    "# ============================================================\n",
    "\n",
    "def call_llm(system_prompt, user_prompt, response_format=None):\n",
    "    \"\"\"\n",
    "    Call the LLM with given prompts.\n",
    "    \n",
    "    Args:\n",
    "        system_prompt: System message for the LLM\n",
    "        user_prompt: User message for the LLM\n",
    "        response_format: Optional Pydantic model for structured output\n",
    "    \n",
    "    Returns:\n",
    "        String response or parsed object if response_format is provided\n",
    "    \"\"\"\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI()\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    if response_format:\n",
    "        # Use structured output\n",
    "        completion = client.chat.completions.parse(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            response_format=response_format\n",
    "        )\n",
    "        return completion.choices[0].message.parsed\n",
    "    else:\n",
    "        # Regular text output\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "\n",
    "# TODO\n",
    "def extract_with_prompting(order):\n",
    "    \"\"\"\n",
    "    Extract order information using text prompts.\n",
    "    \n",
    "    TODO: Write prompts to extract:\n",
    "    - Items ordered (with quantities)\n",
    "    - Special modifications\n",
    "    - Drinks\n",
    "    - Delivery/pickup preference\n",
    "    \"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"TODO: Your system prompt here\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"TODO: Your user prompt here\"\"\"\n",
    "    \n",
    "    return call_llm(system_prompt, user_prompt)\n",
    "\n",
    "# ============================================================\n",
    "# APPROACH 2: JSON Prompting\n",
    "# ============================================================\n",
    "\n",
    "# TODO\n",
    "def extract_with_json(order):\n",
    "    \"\"\"\n",
    "    Extract order information as JSON.\n",
    "    \n",
    "    TODO: Write prompts that return JSON with:\n",
    "    - items: list of items with quantities\n",
    "    - modifications: list of special requests\n",
    "    - drinks: list of drinks\n",
    "    - delivery_type: \"delivery\", \"pickup\", or \"not specified\"\n",
    "    \"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"TODO: Your system prompt here\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"TODO: Your user prompt here\"\"\"\n",
    "    \n",
    "    response = call_llm(system_prompt, user_prompt)\n",
    "    \n",
    "    # Parse the JSON response\n",
    "    try:\n",
    "        return json.loads(response)\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"error\": \"Failed to parse JSON\"}\n",
    "\n",
    "# ============================================================\n",
    "# APPROACH 3: Structured Output with Pydantic\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# TODO\n",
    "class FoodOrder(BaseModel):\n",
    "    \"\"\"\n",
    "    # TODO: Define the structure for a food order.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# TODO\n",
    "def extract_with_structured_output(order):\n",
    "    \"\"\"\n",
    "    Extract order using structured output.\n",
    "    \"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"TODO: Your system prompt here\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"TODO: Your user prompt here\"\"\"\n",
    "    \n",
    "    return call_llm(system_prompt, user_prompt, response_format=FoodOrder) # <=== We are passing the FoodOrder pydantic model here\n",
    "\n",
    "\n",
    "##########################################################\n",
    "##########################################################\n",
    "#     YOU DON'T NEED TO CHANGE ANY OF THE CODE BELOW     #\n",
    "##########################################################\n",
    "##########################################################\n",
    "\n",
    "def compare_approaches():\n",
    "    \"\"\"Process all orders with each approach and compare results.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPARING ALL APPROACHES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i, order in enumerate(food_orders, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ORDER {i}: {order}\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        # Text output\n",
    "        print(\"\\n\ud83d\udcdd Text Output:\")\n",
    "        print(extract_with_prompting(order))\n",
    "        \n",
    "        # JSON output\n",
    "        print(\"\\n\ud83d\udccb JSON Output:\")\n",
    "        json_result = extract_with_json(order)\n",
    "        if isinstance(json_result, dict):\n",
    "            print(json.dumps(json_result, indent=2))\n",
    "        else:\n",
    "            print(json_result)\n",
    "        \n",
    "        # Structured output\n",
    "        print(\"\\n\ud83c\udfaf Structured Output:\")\n",
    "        structured_result = extract_with_structured_output(order)\n",
    "        if hasattr(structured_result, 'model_dump_json'):\n",
    "            print(structured_result.model_dump_json(indent=2))\n",
    "        else:\n",
    "            print(structured_result)\n",
    "\n",
    "# Run comparison\n",
    "compare_approaches()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e310e755",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Click to reveal solution</strong></summary>\n",
    "\n",
    "```python\n",
    "import json\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional, Literal\n",
    "\n",
    "# Sample food orders - all contain similar info in different formats\n",
    "food_orders = [\n",
    "    \"2 pepperoni pizzas large, 1 greek salad, and 3 cokes for delivery to 123 Main St\",\n",
    "    \"One burger medium with no onions, extra cheese, side of fries and a chocolate shake\",\n",
    "    \"3x chicken tacos, 1x beef burrito no beans, 2 large horchatas, extra hot sauce please\",\n",
    "    \"Pad Thai mild spice, Tom Yum soup, 2 spring rolls, and jasmine tea for pickup\",\n",
    "    \"Large coffee with oat milk and 2 sugars, plus a blueberry muffin warmed up\",\n",
    "    \"Family meal: 1 whole roast chicken, mashed potatoes, coleslaw, 4 dinner rolls\"\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# LLM HELPER FUNCTION\n",
    "# ============================================================\n",
    "\n",
    "def call_llm(system_prompt, user_prompt, response_format=None):\n",
    "    \"\"\"\n",
    "    Call the LLM with given prompts.\n",
    "    \n",
    "    Args:\n",
    "        system_prompt: System message for the LLM\n",
    "        user_prompt: User message for the LLM\n",
    "        response_format: Optional Pydantic model for structured output\n",
    "    \n",
    "    Returns:\n",
    "        String response or parsed object if response_format is provided\n",
    "    \"\"\"\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI()\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    if response_format:\n",
    "        # Use structured output\n",
    "        completion = client.chat.completions.parse(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            response_format=response_format\n",
    "        )\n",
    "        return completion.choices[0].message.parsed\n",
    "    else:\n",
    "        # Regular text output\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "\n",
    "def extract_with_prompting(order):\n",
    "    \"\"\"\n",
    "    Extract order information using text prompts.\n",
    "    \n",
    "    TODO: Write prompts to extract:\n",
    "    - Items ordered (with quantities)\n",
    "    - Special modifications\n",
    "    - Drinks\n",
    "    - Delivery/pickup preference\n",
    "    \"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"You are a food order parser. Extract information from food orders and format it clearly.\n",
    "    Include: items with quantities, modifications, drinks, and delivery preference.\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"Extract the following from this order:\n",
    "    - Food items (with quantities)\n",
    "    - Special modifications or requests\n",
    "    - Drinks (if any)\n",
    "    - Delivery or pickup preference (if mentioned)\n",
    "    \n",
    "    Order: {order}\n",
    "    \n",
    "    Format your response as:\n",
    "    ITEMS: \n",
    "    MODIFICATIONS:\n",
    "    DRINKS:\n",
    "    DELIVERY TYPE:\"\"\"\n",
    "    \n",
    "    return call_llm(system_prompt, user_prompt)\n",
    "\n",
    "# ============================================================\n",
    "# APPROACH 2: JSON Prompting\n",
    "# ============================================================\n",
    "\n",
    "def extract_with_json(order):\n",
    "    \"\"\"\n",
    "    Extract order information as JSON.\n",
    "    \n",
    "    TODO: Write prompts that return JSON with:\n",
    "    - items: list of items with quantities\n",
    "    - modifications: list of special requests\n",
    "    - drinks: list of drinks\n",
    "    - delivery_type: \"delivery\", \"pickup\", or \"not specified\"\n",
    "    \"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"You are a food order parser. Extract information from food orders and return ONLY valid JSON.\n",
    "    No additional text or explanation, just the JSON object.\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"Extract information from this order and return as JSON with this structure:\n",
    "    {{\n",
    "        \"items\": [\"list of food items with quantities\"],\n",
    "        \"modifications\": [\"list of special requests or modifications\"],\n",
    "        \"drinks\": [\"list of drinks ordered\"],\n",
    "        \"delivery_type\": \"delivery, pickup, or not specified\"\n",
    "    }}\n",
    "    \n",
    "    Order: {order}\"\"\"\n",
    "    \n",
    "    response = call_llm(system_prompt, user_prompt)\n",
    "    \n",
    "    # Parse the JSON response\n",
    "    try:\n",
    "        return json.loads(response)\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"error\": \"Failed to parse JSON\"}\n",
    "\n",
    "# ============================================================\n",
    "# APPROACH 3: Structured Output with Pydantic\n",
    "# ============================================================\n",
    "\n",
    "class OrderItem(BaseModel):\n",
    "    item: str\n",
    "    quantity: int\n",
    "\n",
    "class FoodOrder(BaseModel):\n",
    "    \"\"\"\n",
    "    Define the structure for a food order.\n",
    "    \"\"\"\n",
    "    items: List[OrderItem]  # List of food items with quantities\n",
    "    modifications: List[str]  # Special requests or modifications\n",
    "    drinks: List[OrderItem]  # Drinks ordered\n",
    "    delivery_type: Literal[\"delivery\", \"pickup\", \"not specified\"]  # \"delivery\", \"pickup\", or \"not specified\"\n",
    "\n",
    "def extract_with_structured_output(order):\n",
    "    \"\"\"\n",
    "    Extract order using structured output.\n",
    "    \"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"You are a food order parser. Extract information from food orders.\n",
    "    Be precise with quantities and capture all special modifications.\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"Extract all information from this food order:\n",
    "    \n",
    "    Order: {order}\n",
    "    \n",
    "    Note: For delivery_type, use \"delivery\", \"pickup\", or \"not specified\" if not mentioned.\"\"\"\n",
    "    \n",
    "    return call_llm(system_prompt, user_prompt, response_format=FoodOrder)\n",
    "\n",
    "\n",
    "##########################################################\n",
    "##########################################################\n",
    "#     YOU DON'T NEED TO CHANGE ANY OF THE CODE BELOW     #\n",
    "##########################################################\n",
    "##########################################################\n",
    "def compare_approaches():\n",
    "    \"\"\"Process all orders with each approach and compare results.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPARING ALL APPROACHES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i, order in enumerate(food_orders, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ORDER {i}: {order}\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        # Text output\n",
    "        print(\"\\n\ud83d\udcdd Text Output:\")\n",
    "        print(extract_with_prompting(order))\n",
    "        \n",
    "        # JSON output\n",
    "        print(\"\\n\ud83d\udccb JSON Output:\")\n",
    "        json_result = extract_with_json(order)\n",
    "        if isinstance(json_result, dict):\n",
    "            print(json.dumps(json_result, indent=2))\n",
    "        else:\n",
    "            print(json_result)\n",
    "        \n",
    "        # Structured output\n",
    "        print(\"\\n\ud83c\udfaf Structured Output:\")\n",
    "        structured_result = extract_with_structured_output(order)\n",
    "        if hasattr(structured_result, 'model_dump_json'):\n",
    "            print(structured_result.model_dump_json(indent=2))\n",
    "        else:\n",
    "            print(structured_result)\n",
    "\n",
    "# Run comparison\n",
    "compare_approaches()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b576be",
   "metadata": {},
   "source": [
    "### Exercise 3: Tool Selection\n",
    "\n",
    "Make an LLM select which function to call based on user input using structured outputs.\n",
    "\n",
    "Your task:\n",
    "1. **Define a Pydantic model** to output the selected function name\n",
    "2. **Write prompts** that help the LLM choose the right function\n",
    "3. **Test your routing** with various user inputs\n",
    "\n",
    "Available functions: `get_time`, `get_weather`, `get_joke`, `get_fact`, `calculate`\n",
    "\n",
    "Test inputs include:\n",
    "- Direct requests (\"What time is it?\")\n",
    "- Indirect requests (\"Tell me something funny\")\n",
    "- Ambiguous inputs (\"I'm bored\")\n",
    "\n",
    "*Just focus on the sections marked as **TODO** for this exercise, our focus is just on prompting for now. The function selected by the LLM is automatically run by the code given below.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e856483-dd88-4578-80d8-31c2c8f32f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "\n",
    "# ============================================================\n",
    "# AVAILABLE FUNCTIONS \n",
    "# ============================================================\n",
    "\n",
    "def get_time():\n",
    "    \"\"\"Returns current time\"\"\"\n",
    "    return \"It's 2:30 PM\"\n",
    "\n",
    "def get_weather():\n",
    "    \"\"\"Returns weather information\"\"\"\n",
    "    return \"It's 72\u00b0F and sunny\"\n",
    "\n",
    "def get_joke():\n",
    "    \"\"\"Tells a funny joke\"\"\"\n",
    "    return \"Why do programmers prefer dark mode? Because light attracts bugs!\"\n",
    "\n",
    "def get_fact():\n",
    "    \"\"\"Returns an interesting fact\"\"\"\n",
    "    return \"Python was named after Monty Python, not the snake!\"\n",
    "\n",
    "def calculate():\n",
    "    \"\"\"Performs a calculation\"\"\"\n",
    "    return \"The answer is 42\"\n",
    "\n",
    "# Function registry (DO NOT MODIFY)\n",
    "FUNCTIONS = {\n",
    "    \"get_time\": get_time,\n",
    "    \"get_weather\": get_weather,\n",
    "    \"get_joke\": get_joke,\n",
    "    \"get_fact\": get_fact,\n",
    "    \"calculate\": calculate\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# YOUR TASK: Function Selection\n",
    "# ============================================================\n",
    "\n",
    "# TODO: Define a Pydantic model for function selection\n",
    "class SelectedFunction(BaseModel):\n",
    "    \"\"\"\n",
    "    TODO: Define what the LLM should output when selecting a function.\n",
    "    What field(s) do you need?\n",
    "    \"\"\"\n",
    "    pass  # Remove this and add your fields\n",
    "\n",
    "def select_function(user_input):\n",
    "    \"\"\"\n",
    "    Use an LLM to select which function should be called.\n",
    "    \n",
    "    TODO: \n",
    "    1. Write prompts to help the LLM understand the user's request\n",
    "    2. Return a FunctionCall object with the selected function\n",
    "    \n",
    "    Think: How will the LLM know which functions are available?\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Write your system prompt\n",
    "    system_prompt = \"\"\"TODO: Your system prompt here\"\"\"\n",
    "    \n",
    "    # TODO: Write your user prompt\n",
    "    user_prompt = f\"\"\"TODO: Your prompt here\n",
    "    \n",
    "    User input: {user_input}\n",
    "    \"\"\"\n",
    "    \n",
    " \n",
    "    selected_function = call_llm(system_prompt, user_prompt, SelectedFunction)\n",
    "    return selected_function\n",
    "\n",
    "# ============================================================\n",
    "# TEST YOUR CODE\n",
    "# ============================================================\n",
    "\n",
    "def test_routing(user_input):\n",
    "    \"\"\"Tests your function selection and executes the selected function.\"\"\"\n",
    "    print(f\"\\n\ud83d\udcac Input: {user_input}\")\n",
    "    \n",
    "    try:\n",
    "        # Get function selection from your code\n",
    "        result = select_function(user_input)\n",
    "        \n",
    "        # Extract function name (adapt based on your model)\n",
    "        if hasattr(result, 'function_name'):\n",
    "            func_name = result.function_name\n",
    "        elif hasattr(result, 'function'):\n",
    "            func_name = result.function\n",
    "        elif hasattr(result, 'name'):\n",
    "            func_name = result.name\n",
    "        else:\n",
    "            print(\"\u274c Couldn't find function name in response\")\n",
    "            return\n",
    "        \n",
    "        # Execute the selected function\n",
    "        if func_name in FUNCTIONS:\n",
    "            output = FUNCTIONS[func_name]()\n",
    "            print(f\"\u2705 Called {func_name}() \u2192 {output}\")\n",
    "        else:\n",
    "            print(f\"\u274c Unknown function: {func_name}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Error: {e}\")\n",
    "\n",
    "# Test cases\n",
    "test_inputs = [\n",
    "    \"What time is it?\",\n",
    "    \"Tell me something funny\",\n",
    "    \"How's the weather?\",\n",
    "    \"Calculate something for me\",\n",
    "    \"Share an interesting fact\",\n",
    "    \"I'm bored\",  # Ambiguous\n",
    "    \"Help me with math\",  # Should select calculate\n",
    "    \"Is it raining?\",  # Weather-related\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Testing Function Routing\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for user_input in test_inputs:\n",
    "    test_routing(user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbe723b",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Click to reveal solution</strong></summary>\n",
    "\n",
    "```python\n",
    "import json\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "\n",
    "# ============================================================\n",
    "# AVAILABLE FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def get_time():\n",
    "    \"\"\"Returns current time\"\"\"\n",
    "    return \"It's 2:30 PM\"\n",
    "\n",
    "def get_weather():\n",
    "    \"\"\"Returns weather information\"\"\"\n",
    "    return \"It's 72\u00b0F and sunny\"\n",
    "\n",
    "def get_joke():\n",
    "    \"\"\"Tells a funny joke\"\"\"\n",
    "    return \"Why do programmers prefer dark mode? Because light attracts bugs!\"\n",
    "\n",
    "def get_fact():\n",
    "    \"\"\"Returns an interesting fact\"\"\"\n",
    "    return \"Python was named after Monty Python, not the snake!\"\n",
    "\n",
    "def calculate():\n",
    "    \"\"\"Performs a calculation\"\"\"\n",
    "    return \"The answer is 42\"\n",
    "\n",
    "FUNCTIONS = {\n",
    "    \"get_time\": get_time,\n",
    "    \"get_weather\": get_weather,\n",
    "    \"get_joke\": get_joke,\n",
    "    \"get_fact\": get_fact,\n",
    "    \"calculate\": calculate\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# YOUR TASK: Function Selection\n",
    "# ============================================================\n",
    "\n",
    "class SelectedFunction(BaseModel):\n",
    "    \"\"\"Model for function selection output.\"\"\"\n",
    "    function_name: Literal[\"get_time\", \"get_weather\", \"get_joke\", \"get_fact\", \"calculate\"]\n",
    "\n",
    "def select_function(user_input):\n",
    "    \"\"\"\n",
    "    Use an LLM to select which function should be called.\n",
    "    \"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"You are a function router. Based on user input, select the most appropriate function to call.\n",
    "    Be precise in matching user intent to function capabilities.\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"Select the appropriate function for this user input.\n",
    "    \n",
    "    Available functions:\n",
    "    - get_time: Returns the current time\n",
    "    - get_weather: Returns weather information  \n",
    "    - get_joke: Tells a funny joke\n",
    "    - get_fact: Returns an interesting fact\n",
    "    - calculate: Performs a calculation\n",
    "    \n",
    "    User input: {user_input}\n",
    "    \n",
    "    Select the most appropriate function based on what the user is asking for.\"\"\"\n",
    "    \n",
    "    selected_function = call_llm(system_prompt, user_prompt, SelectedFunction)\n",
    "    return selected_function\n",
    "\n",
    "# ============================================================\n",
    "# TEST YOUR FUNCTION SELECTION\n",
    "# ============================================================\n",
    "\n",
    "def test_routing(user_input):\n",
    "    \"\"\"Tests your function selection and executes the selected function.\"\"\"\n",
    "    print(f\"\\n\ud83d\udcac Input: {user_input}\")\n",
    "    \n",
    "    try:\n",
    "        # Get function selection from your code\n",
    "        result = select_function(user_input)\n",
    "        \n",
    "        # Extract function name (adapt based on your model)\n",
    "        if hasattr(result, 'function_name'):\n",
    "            func_name = result.function_name\n",
    "        elif hasattr(result, 'function'):\n",
    "            func_name = result.function\n",
    "        elif hasattr(result, 'name'):\n",
    "            func_name = result.name\n",
    "        else:\n",
    "            print(\"\u274c Couldn't find function name in response\")\n",
    "            return\n",
    "        \n",
    "        # Execute the selected function\n",
    "        if func_name in FUNCTIONS:\n",
    "            output = FUNCTIONS[func_name]()\n",
    "            print(f\"\u2705 Called {func_name}() \u2192 {output}\")\n",
    "        else:\n",
    "            print(f\"\u274c Unknown function: {func_name}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Error: {e}\")\n",
    "\n",
    "# Test cases\n",
    "test_inputs = [\n",
    "    \"What time is it?\",\n",
    "    \"Tell me something funny\",\n",
    "    \"How's the weather?\",\n",
    "    \"Calculate something for me\",\n",
    "    \"Share an interesting fact\",\n",
    "    \"I'm bored\",  # Ambiguous\n",
    "    \"Help me with math\",  # Should select calculate\n",
    "    \"Is it raining?\",  # Weather-related\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Testing Function Routing\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for user_input in test_inputs:\n",
    "    test_routing(user_input)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3969db0e",
   "metadata": {},
   "source": [
    "### Food for thought after Exercise 3\n",
    "\n",
    "1. What information did you include in your prompts?\n",
    "2. How did the LLM know which function to select?\n",
    "3. What happened with ambiguous inputs like \"I'm bored\"?\n",
    "4. Would this work if we had 100 functions instead of 5?\n",
    "5. What if the functions have input parameters?\n",
    "\n",
    "These will be important for the next workshop when we start building our AI agent!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967b5b11",
   "metadata": {},
   "source": [
    "## Summary: From Prompts to Agents\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "**Part 1: Fundamental Techniques**\n",
    "1. **Zero-shot**: Quick but unreliable\n",
    "2. **Few-shot**: Pattern teaching through examples\n",
    "3. **Chain-of-Thought**: Step-by-step reasoning for better accuracy\n",
    "\n",
    "**Part 2: Agent Capabilities**\n",
    "4. **System Prompts**: Define behavior and constraints\n",
    "5. **Output Format Control**: Ensure parseable responses\n",
    "6. **Error Handling**: Automatic retry and correction\n",
    "7. **Tool Selection**: Automatically identify the right tool that needs to be called for a task\n",
    "\n",
    "### Next Week\n",
    "\n",
    "1. Introduction to Agents\n",
    "2. ReAct prompting\n",
    "3. Identify building blocks for Agents\n",
    "4. Start building out the agent from scratch\n",
    "\n",
    "### Useful Resources\n",
    "\n",
    "**[Andrej Karpathy's Neural Networks: Zero to Hero Playlist](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&ab_channel=AndrejKarpathy)**:\n",
    "Although the title says Neural Networks, it actually starts from basics of Neural Networks, and builds up all the way to Large Language Models, all from scratch. For those interested in the technical details of exactly how LLMs are built, this playlist is great!\n",
    "\n",
    "**[Prompt Engineering Guide](https://www.promptingguide.ai/)**:\n",
    "This site contains examples of the many prompt engineering techniques we covered today, along with additional ones that are often used when building agents (eg. ReAct)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}