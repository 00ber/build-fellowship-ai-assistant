{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab4112f1-3918-4acf-b26e-442f69233d9b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a9fc78f-cbc5-4f8d-8e8c-03df91c6ca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "def generate(prompt):\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful and unbiased assistant.\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2a65a9-3cde-42e2-9b37-da2b2a953605",
   "metadata": {},
   "source": [
    "# Part 1: Fundamental Prompting Techniques\n",
    "\n",
    "In this section, we'll explore the core prompting strategies that form the foundation of working with LLMs. We'll use a consistent task across all examples so you can clearly see how each technique affects the model's behavior.\n",
    "\n",
    "## 1. Zero-shot Prompting\n",
    "\n",
    "**What is it?** Zero-shot prompting means asking the model to perform a task without any examples. You're relying entirely on the model's pre-trained knowledge.\n",
    "\n",
    "**When to use it:**\n",
    "- Simple, well-defined tasks\n",
    "- When you want quick results without setup\n",
    "\n",
    "**Watch what happens:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a60656b-f70e-40be-b103-cb516fa81be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Create an acronym from 'artificial neural network' using the last letter of each word.\n",
      "\n",
      "Output: The last letters of the words \"artificial neural network\" are \"l,\" \"a,\" and \"k.\" An acronym using these letters could be \"LAK.\"\n"
     ]
    }
   ],
   "source": [
    "# Zero-shot: No examples given\n",
    "prompt = \"Create an acronym from 'artificial neural network' using the last letter of each word.\"\n",
    "zero_shot_output = generate(prompt)\n",
    "\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"\\nOutput:\", zero_shot_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c45e717-ef64-45d5-a75b-a94051c6db11",
   "metadata": {},
   "source": [
    "## 2. One-shot Prompting\n",
    "\n",
    "**What is it?** One-shot prompting provides an example before asking the model to perform the task. This helps the model understand the pattern you want.\n",
    "\n",
    "**When to use it:**\n",
    "- Tasks with specific formats\n",
    "- When zero-shot doesn't work well\n",
    "- Teaching a simple pattern quickly\n",
    "\n",
    "**Watch what happens::**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdb5cf14-f0f8-4e2f-b50e-561b070eeb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Create an acronym using the last letter of each word:\n",
      "\n",
      "\"big red car\" -> GDR\n",
      "\"artificial neural network\" ->\n",
      "\n",
      "Output: The acronym for \"artificial neural network\" using the last letter of each word is: LAL.\n"
     ]
    }
   ],
   "source": [
    "# One-shot: One example provided\n",
    "prompt = \"\"\"Create an acronym using the last letter of each word:\n",
    "\n",
    "\"big red car\" -> GDR\n",
    "\"artificial neural network\" ->\"\"\"\n",
    "\n",
    "one_shot_output = generate(prompt)\n",
    "\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"\\nOutput:\", one_shot_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4df89ca-4d3a-43a8-877f-aa43f86b0df4",
   "metadata": {},
   "source": [
    "## 3. Few-shot Prompting\n",
    "\n",
    "**What is it?** Few-shot prompting provides multiple examples (typically 2-5) to establish a clear pattern. The model learns from these examples and applies the pattern to new inputs.\n",
    "\n",
    "**When to use it:**\n",
    "- Complex patterns that need reinforcement\n",
    "- Tasks requiring specific formatting\n",
    "- When consistency is critical\n",
    "\n",
    "**Multiple examples = stronger pattern:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9eb5145-48ea-4b6f-8bf2-631e03992172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Create an acronym using the last letter of each word:\n",
      "\n",
      "\"big red car\" -> GDR\n",
      "\"machine learning system\" -> EGM\n",
      "\"deep learning model\" -> PGL\n",
      "\"artificial neural network\" ->\n",
      "\n",
      "Output: The acronym for \"artificial neural network\" using the last letter of each word is: KAL.\n"
     ]
    }
   ],
   "source": [
    "# Few-shot: Multiple examples provided\n",
    "prompt = \"\"\"Create an acronym using the last letter of each word:\n",
    "\n",
    "\"big red car\" -> GDR\n",
    "\"machine learning system\" -> EGM\n",
    "\"deep learning model\" -> PGL\n",
    "\"artificial neural network\" ->\"\"\"\n",
    "\n",
    "few_shot_output = generate(prompt)\n",
    "\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"\\nOutput:\", few_shot_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc438da4-168a-4b77-bd93-391d9365bcf7",
   "metadata": {},
   "source": [
    "## 4. Zero-shot Chain-of-Thought (CoT)\n",
    "\n",
    "**What is it?** By adding \"Think step-by-step\" or similar phrases, we trigger the model to show its reasoning process. This often leads to more accurate results, especially for tasks requiring logic or calculation.\n",
    "\n",
    "**When to use it:**\n",
    "- Tasks requiring the model to reasoning correctly\n",
    "- When you need to verify the model's thinking\n",
    "\n",
    "**Magic phrase: \"Think step-by-step\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c6b7aa4-723b-4702-be14-2511a864c69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Create an acronym from 'artificial neural network' using the last letter of each word. Think step-by-step.\n",
      "\n",
      "Output: To create an acronym from the phrase \"artificial neural network\" using the last letter of each word, we can follow these steps:\n",
      "\n",
      "1. Identify the last letter of each word:\n",
      "   - \"artificial\" ‚Üí last letter is **l**\n",
      "   - \"neural\" ‚Üí last letter is **l**\n",
      "   - \"network\" ‚Üí last letter is **k**\n",
      "\n",
      "2. Combine the last letters:\n",
      "   - From \"artificial\" ‚Üí **l**\n",
      "   - From \"neural\" ‚Üí **l**\n",
      "   - From \"network\" ‚Üí **k**\n",
      "\n",
      "3. Form the acronym:\n",
      "   - The acronym formed from the last letters is **LLK**.\n",
      "\n",
      "So, the acronym from \"artificial neural network\" using the last letter of each word is **LLK**.\n"
     ]
    }
   ],
   "source": [
    "# Zero-shot CoT: No examples, but asking for reasoning\n",
    "prompt = \"Create an acronym from 'artificial neural network' using the last letter of each word. Think step-by-step.\"\n",
    "\n",
    "zero_shot_cot_output_1 = generate(prompt)\n",
    "\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"\\nOutput:\", zero_shot_cot_output_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc29d7f4-4342-4315-b8ad-cc09d023fc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Create an acronym from 'artificial neural network' using the last letter of each word. Before answering, first explain your work!\n",
      "\n",
      "Output: To create an acronym from the phrase \"artificial neural network,\" we will take the last letter of each word in the phrase. \n",
      "\n",
      "1. The last letter of \"artificial\" is **l**.\n",
      "2. The last letter of \"neural\" is **l**.\n",
      "3. The last letter of \"network\" is **k**.\n",
      "\n",
      "Now, we will combine these letters to form the acronym. \n",
      "\n",
      "So, the acronym formed from the last letters of \"artificial neural network\" is **LLK**.\n"
     ]
    }
   ],
   "source": [
    "# Another example\n",
    "prompt = \"Create an acronym from 'artificial neural network' using the last letter of each word. Before answering, first explain your work!\"\n",
    "\n",
    "zero_shot_cot_output_2 = generate(prompt)\n",
    "\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"\\nOutput:\", zero_shot_cot_output_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6ad9a4-11be-48d9-aaed-ae5594a4fe1c",
   "metadata": {},
   "source": [
    "## 5. Few-Shot Chain-of-Thought (CoT)\n",
    "\n",
    "**What is it?** A powerful combination - providing examples that demonstrate step-by-step reasoning. The model learns both the pattern AND the reasoning process.\n",
    "\n",
    "**When to use it:**\n",
    "- Complex tasks requiring specific reasoning\n",
    "- Teaching the model a particular problem-solving approach\n",
    "- Maximum accuracy is needed\n",
    "\n",
    "**Examples + Reasoning = Very good results:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "938f249b-e657-428d-ba7c-b6dfc762bd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Create an acronym using the last letter of each word. Think step-by-step.\n",
      "\n",
      "Here are some examples:\n",
      "\n",
      "Example 1: \n",
      "Phrase: \"big red car\"\n",
      "- Words: big, red, car\n",
      "- Last letters: g, d, r\n",
      "- Result: GDR\n",
      "\n",
      "Example 2: \n",
      "Phrase: \"machine learning system\"\n",
      "- Words: machine, learning, system\n",
      "- Last letters: e, g, m\n",
      "- Result: EGM\n",
      "\n",
      "Now solve: \n",
      "Phrase: \"artificial neural network\"\n",
      "\n",
      "\n",
      "Output: Let's break down the phrase \"artificial neural network\" step-by-step:\n",
      "\n",
      "1. Identify the words: \n",
      "   - artificial\n",
      "   - neural\n",
      "   - network\n",
      "\n",
      "2. Find the last letter of each word:\n",
      "   - artificial ‚Üí l\n",
      "   - neural ‚Üí l\n",
      "   - network ‚Üí k\n",
      "\n",
      "3. Combine the last letters:\n",
      "   - Last letters: l, l, k\n",
      "\n",
      "4. Resulting acronym:\n",
      "   - LLK\n",
      "\n",
      "So, the acronym for \"artificial neural network\" is **LLK**.\n"
     ]
    }
   ],
   "source": [
    "# Few-shot CoT: Examples with step-by-step reasoning\n",
    "prompt = \"\"\"Create an acronym using the last letter of each word. Think step-by-step.\n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "Example 1: \n",
    "Phrase: \"big red car\"\n",
    "- Words: big, red, car\n",
    "- Last letters: g, d, r\n",
    "- Result: GDR\n",
    "\n",
    "Example 2: \n",
    "Phrase: \"machine learning system\"\n",
    "- Words: machine, learning, system\n",
    "- Last letters: e, g, m\n",
    "- Result: EGM\n",
    "\n",
    "Now solve: \n",
    "Phrase: \"artificial neural network\"\n",
    "\"\"\"\n",
    "\n",
    "few_shot_cot_output = generate(prompt)\n",
    "\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"\\nOutput:\", few_shot_cot_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2rvo0o9npl",
   "metadata": {},
   "source": [
    "## Comparison:\n",
    "\n",
    "Let's compare all techniques side by side to see how each approach affects the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0lw2irnc5kxr",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç COMPARISON:\n",
      "\n",
      "==================================================\n",
      "Zero-shot       ‚Üí The last letters of the words \"artificial neural network\" are \"l,\" \"a,\" and \"k.\" An acronym using these letters could be \"LAK.\"\n",
      "One-shot        ‚Üí The acronym for \"artificial neural network\" using the last letter of each word is: LAL.\n",
      "Few-shot        ‚Üí The acronym for \"artificial neural network\" using the last letter of each word is: KAL.\n",
      "Zero-shot CoT (example 1) ‚Üí So, the acronym from \"artificial neural network\" using the last letter of each word is **LLK**.\n",
      "Zero-shot CoT (example 2) ‚Üí So, the acronym formed from the last letters of \"artificial neural network\" is **LLK**.\n",
      "Few-shot CoT    ‚Üí So, the acronym for \"artificial neural network\" is **LLK**.\n"
     ]
    }
   ],
   "source": [
    "# Compare all techniques\n",
    "print(\"üîç COMPARISON:\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "techniques = [\n",
    "    (\"Zero-shot\", zero_shot_output),\n",
    "    (\"One-shot\", one_shot_output),\n",
    "    (\"Few-shot\", few_shot_output),\n",
    "    (\"Zero-shot CoT (example 1)\", zero_shot_cot_output_1.split('\\n')[-1] if '\\n' in zero_shot_cot_output_1 else zero_shot_cot_output_1),\n",
    "    (\"Zero-shot CoT (example 2)\", zero_shot_cot_output_2.split('\\n')[-1] if '\\n' in zero_shot_cot_output_2 else zero_shot_cot_output_2),\n",
    "    (\"Few-shot CoT\", few_shot_cot_output.split('\\n')[-1] if '\\n' in few_shot_cot_output else few_shot_cot_output)\n",
    "]\n",
    "\n",
    "for name, output in techniques:\n",
    "    # Extract just the answer for cleaner comparison\n",
    "    answer = output.split(\"**\")[-1] if \"**\" in output else output\n",
    "    print(f\"{name:15} ‚Üí {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tv7l2jbychg",
   "metadata": {},
   "source": [
    "# Part 2: Beyond Text Generation\n",
    "\n",
    "So far, we've seen that LLMs are fundamentally:\n",
    "**Text Input ‚Üí LLM ‚Üí Text Output**\n",
    "\n",
    "But what if we could do more? Let me introduce a series of \"what if\" questions. Each question will unlock a new capability, gradually revealing how simple text generators can evolve into more complex things."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe9707f-6313-4555-82a2-72ad32c20fc0",
   "metadata": {},
   "source": [
    "### ü§î What if we want to change the LLM's behavior or personality?\n",
    "\n",
    "Right now, every time we use an LLM, it responds with its default personality and style. But what if we could give it a specific role or personality? What if we could make it behave like a Python expert, or a data scientist, or even a pirate?\n",
    "\n",
    "**Enter: System Prompts!**\n",
    "\n",
    "#### Let's take a look at a few examples of using system prompts to change behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bnxweaz0z9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function with system prompt\n",
    "def generate_with_system(system_prompt, user_prompt, temperature=0):\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ud64b1sj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same question, different roles:\n",
      "\n",
      "üé≠ Pirate:\n",
      "Arrr, matey! Large Language Models be mighty algorithms that learn from vast seas o' text, usin' patterns to generate human-like speech. They be helpin' with tasks like writin', translatin', and answerin' questions, all while sailin' the treacherous waters of language! Yarrr!\n",
      "\n",
      "üé≠ Shakespeare:\n",
      "In realms of code and thought entwined,  \n",
      "Large Language Models, vast and refined,  \n",
      "With words they weave, like silken thread,  \n",
      "From countless tomes, their wisdom spread.  \n",
      "They mimic speech, both wise and fair,  \n",
      "A mirror of our minds laid bare.\n",
      "\n",
      "üé≠ Data Scientist:\n",
      "Large Language Models (LLMs) are advanced neural networks, typically based on transformer architecture, trained on vast text corpora to understand and generate human-like language. They leverage unsupervised learning and fine-tuning to perform various natural language processing tasks, including text generation, translation, and sentiment analysis.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare different roles\n",
    "roles = {\n",
    "    \"Pirate\": \"You are a pirate. You always respond in pirate speak.\",\n",
    "    \"Shakespeare\": \"You are Shakespeare. Respond in Elizabethan English with poetic flair.\",\n",
    "    \"Data Scientist\": \"You are a data scientist. Use technical terms and be precise.\"\n",
    "}\n",
    "\n",
    "user_prompt = \"Explain what Large Language Models are in less than 50 words.\"\n",
    "\n",
    "print(\"Same question, different roles:\\n\")\n",
    "for role_name, system_prompt in roles.items():\n",
    "    output = generate_with_system(system_prompt, user_prompt)\n",
    "    print(f\"üé≠ {role_name}:\")\n",
    "    print(output + \"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5yegjqshw2t",
   "metadata": {},
   "source": [
    "**üí° Insight:** System prompts are like giving the LLM a job description! They define how it should behave, what expertise it should have, and what constraints it should follow. This is the first step toward making LLMs useful for specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d6cf96-9665-4456-8b24-5e33c90c459a",
   "metadata": {},
   "source": [
    "### ü§î LLM outputs change from one run to another... What if we want something more deterministic? \n",
    "### What if we want the output to be of a specific format? Or follow certain rules (eg. \"month\" should contain only one of the following words: \"Jan, Feb, Mar, Apr, May, Jun, July, ...... Dec\")?\n",
    "\n",
    "You might have noticed that LLMs sometimes give different answers to the same question, more often when the temperature is high. This randomness can be a problem when we need consistent, reliable outputs. What if we could control not just WHAT the LLM says, but HOW it structures its response?\n",
    "\n",
    "#### Let's see if we can prompt our way to a fixed format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67mpggb22ks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON Output:\n",
      "{\n",
      "  \"participants\": [\"Alice\", \"Bob\"],\n",
      "  \"time\": \"2 PM\",\n",
      "  \"location\": \"Italian restaurant downtown\"\n",
      "}\n",
      "\n",
      "‚úÖ Valid JSON! Parsed data: {'participants': ['Alice', 'Bob'], 'time': '2 PM', 'location': 'Italian restaurant downtown'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Example 1: JSON output\n",
    "json_prompt = \"\"\"Extract information from this text and return ONLY valid JSON:\n",
    "\n",
    "\"Alice and Bob are meeting for lunch tomorrow at 2 PM at the Italian restaurant downtown.\"\n",
    "\n",
    "Format:\n",
    "{\n",
    "  \"participants\": [...],\n",
    "  \"time\": \"...\",\n",
    "  \"location\": \"...\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "output = generate(json_prompt)\n",
    "print(\"JSON Output:\")\n",
    "print(output)\n",
    "\n",
    "# Parse it to verify it's valid JSON\n",
    "try:\n",
    "    data = json.loads(output)\n",
    "    print(\"\\n‚úÖ Valid JSON! Parsed data:\", data)\n",
    "except json.JSONDecodeError as e:\n",
    "    print(\"\\n‚ùå Invalid JSON\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd3bc5a-1436-44bf-9537-ddd101d4c887",
   "metadata": {},
   "source": [
    "OK, that worked! But this can be brittle. There is a better way.\n",
    "\n",
    "### Recommended method for generating structured output\n",
    "While prompting to output specific format may give the desired output a lot of the time, most LLMs these days have the \"Structured Output\" feature. It allows us to provide an output format via JSON Schema or Pydantic (a Python data validation library that uses type annotations to define schemas). These methods guarantee that the output will conform to your specified structure, eliminating parsing errors and invalid formats.\n",
    "\n",
    "Eg: https://platform.openai.com/docs/guides/structured-outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "x7qqn0rqif",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured Output with Pydantic:\n",
      "Event Name: Science Fair\n",
      "Event Date: Friday\n",
      "Participants: ['Alice', 'Bob']\n",
      "\n",
      "üéØ Guaranteed valid structure!\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Structured output with Pydantic (OpenAI's structured outputs)\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "completion = client.chat.completions.parse(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n",
    "    ],\n",
    "    response_format=CalendarEvent,\n",
    ")\n",
    "\n",
    "event = completion.choices[0].message.parsed\n",
    "\n",
    "print(\"Structured Output with Pydantic:\")\n",
    "print(f\"Event Name: {event.name}\")\n",
    "print(f\"Event Date: {event.date}\")\n",
    "print(f\"Participants: {event.participants}\")\n",
    "print(\"\\nüéØ Guaranteed valid structure!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lhpa2oeao3",
   "metadata": {},
   "source": [
    "**üí° Insight:** We can force LLMs to output in specific formats like JSON or use Pydantic schemas to guarantee structure. This makes their outputs predictable and parseable by code!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1c6c76-fd77-47ea-bc41-4f277ed2eefe",
   "metadata": {},
   "source": [
    "### ü§î Since LLMs can output text, can we actually output code?\n",
    "\n",
    "We've seen LLMs generate formatted text and JSON. But code is just text too, right? What if we asked an LLM to write actual, executable code?\n",
    "\n",
    "\n",
    "#### Let's find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4x889fphj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Code:\n",
      "==================================================\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "# Step 1: Create a DataFrame with sales data\n",
      "data = {\n",
      "    'product': ['A', 'B', 'C', 'D'],\n",
      "    'price': [10, 20, 15, 30],\n",
      "    'quantity': [100, 150, 200, 50]\n",
      "}\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Step 2: Calculate total revenue\n",
      "df['total_revenue'] = df['price'] * df['quantity']\n",
      "\n",
      "# Step 3: Find the product with highest revenue\n",
      "highest_revenue_product = df.loc[df['total_revenue'].idxmax()]\n",
      "\n",
      "print(highest_revenue_product)\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Ask LLM to generate pandas code\n",
    "prompt = \"\"\"Generate Python code using pandas to:\n",
    "1. Create a DataFrame with sales data (product, price, quantity)\n",
    "2. Calculate total revenue (price * quantity)\n",
    "3. Find the product with highest revenue\n",
    "\n",
    "Output ONLY executable Python code.\"\"\"\n",
    "\n",
    "code_output = generate(prompt)\n",
    "print(\"Generated Code:\")\n",
    "print(\"=\" * 50)\n",
    "print(code_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iw3d9o7xz3",
   "metadata": {},
   "source": [
    "### ü§î If we can output code, what if we actually RUN that code?\n",
    "\n",
    "The LLM just generated code as text. But it's valid Python code... What if we could execute it programmatically? What if our program could take the LLM's code and actually run it?\n",
    "\n",
    "#### Let's try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8az3n298evq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing the LLM-generated code...\n",
      "==================================================\n",
      "product             B\n",
      "price              20\n",
      "quantity          150\n",
      "total_revenue    3000\n",
      "Name: 1, dtype: object\n",
      "\n",
      "üéâ The LLM's code ran successfully!\n"
     ]
    }
   ],
   "source": [
    "# Extract just the code (remove markdown formatting if any)\n",
    "if \"```python\" in code_output:\n",
    "    actual_code = code_output.split(\"```python\")[1].split(\"```\")[0]\n",
    "else:\n",
    "    actual_code = code_output\n",
    "\n",
    "print(\"Executing the LLM-generated code...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Use exec() to run the code\n",
    "import pandas as pd\n",
    "exec(actual_code)\n",
    "\n",
    "print(\"\\nüéâ The LLM's code ran successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gkaj8bkoica",
   "metadata": {},
   "source": [
    "### ü§î We have just run the code that the LLM produced, what if we feed the output back to the LLM and allow the LLM to see the results of its code?\n",
    "\n",
    "So we can generate code and run it. But what happens after? The code produces output... What if we could capture that output and feed it back to the LLM? What if the LLM could see the results of its own code?\n",
    "\n",
    "### Let's try that out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "78s1657kg4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Code:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Generate 5 random numbers\n",
      "random_numbers = np.random.rand(5)\n",
      "\n",
      "# Create a DataFrame\n",
      "df = pd.DataFrame(random_numbers, columns=['Random Numbers'])\n",
      "\n",
      "# Calculate the mean\n",
      "mean_value = df['Random Numbers'].mean()\n",
      "\n",
      "# Print the DataFrame and the mean\n",
      "print(\"DataFrame:\")\n",
      "print(df)\n",
      "print(\"\\nMean of the random numbers:\", mean_value)\n",
      "\n",
      "\n",
      "==================================================\n",
      "Captured Output:\n",
      "DataFrame:\n",
      "   Random Numbers\n",
      "0        0.930563\n",
      "1        0.635088\n",
      "2        0.296343\n",
      "3        0.091544\n",
      "4        0.496927\n",
      "\n",
      "Mean of the random numbers: 0.4900930474608751\n",
      "\n",
      "==================================================\n",
      "\n",
      "LLM Analysis of its own output:\n",
      "The mean value of the random numbers is approximately 0.4901. Since this value is below 0.5, we can conclude that the mean is below 0.5.\n",
      "\n",
      "üîÑ We've closed the loop! The LLM can see and analyze the results of its own code!\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import sys\n",
    "\n",
    "def capture_output(code):\n",
    "    \"\"\"Execute code and capture its output\"\"\"\n",
    "    # Redirect stdout to capture print statements\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = captured_output = io.StringIO()\n",
    "    \n",
    "    try:\n",
    "        exec(code, {'pd': pd})\n",
    "        output = captured_output.getvalue()\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Step 1: Generate code\n",
    "prompt1 = \"Generate pandas code to create a DataFrame with 5 random numbers and calculate their mean. Print the DataFrame and the mean.\"\n",
    "code1 = generate(prompt1)\n",
    "\n",
    "# Clean the code\n",
    "if \"```python\" in code1:\n",
    "    code1 = code1.split(\"```python\")[1].split(\"```\")[0]\n",
    "\n",
    "print(\"Generated Code:\")\n",
    "print(code1)\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# Step 2: Run and capture output\n",
    "output = capture_output(code1)\n",
    "print(\"Captured Output:\")\n",
    "print(output)\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Step 3: Feed output back to LLM\n",
    "prompt2 = f\"\"\"Based on this output from a pandas operation:\n",
    "\n",
    "{output}\n",
    "\n",
    "What is the mean value? Is it above or below 0.5?\"\"\"\n",
    "\n",
    "analysis = generate(prompt2)\n",
    "print(\"\\nLLM Analysis of its own output:\")\n",
    "print(analysis)\n",
    "\n",
    "print(\"\\nüîÑ We've closed the loop! The LLM can see and analyze the results of its own code!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44srm33r2m5",
   "metadata": {},
   "source": [
    "### ü§î The code from the LLM might be written poorly and run into errors. What if feed those back to the LLM and prompt it to fix it?\n",
    "\n",
    "Sometimes the code the LLM generates has errors. But if we can capture those errors and feed them back... What if the LLM could debug its own code? What if it could learn from its mistakes and try again?\n",
    "\n",
    "\n",
    "### Let's intentionally cause an error and see if the LLM can fix it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "vavo27idsqh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1:\n",
      "\n",
      "# Calculate the average grade\n",
      "average_grade = sum(student_grades) / len(student_grades) if student_grades else 0\n",
      "\n",
      "# Print the average grade\n",
      "print(f\"Average Grade: {average_grade:.2f}\")\n",
      "\n",
      "# Print each student with their index and ID\n",
      "for index, student_id in enumerate(student_ids):\n",
      "    print(f\"Index: {index}, Student ID: {student_id}, Grade: {student_grades[index]}\")\n",
      "\n",
      "\n",
      "‚ùå Error: name 'student_grades' is not defined\n",
      "üîÑ Asking LLM to fix it...\n",
      "\n",
      "Attempt 2:\n",
      "\n",
      "# Available context\n",
      "context = {'grades': [85, 92, 78, 95, 88], 'students': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve']}\n",
      "\n",
      "# Extract grades and student IDs from the context\n",
      "student_grades = context['grades']\n",
      "student_ids = context['students']\n",
      "\n",
      "# Calculate the average grade\n",
      "average_grade = sum(student_grades) / len(student_grades) if student_grades else 0\n",
      "\n",
      "# Print the average grade\n",
      "print(f\"Average Grade: {average_grade:.2f}\")\n",
      "\n",
      "# Print each student with their index and ID\n",
      "for index, student_id in enumerate(student_ids):\n",
      "    print(f\"Index: {index}, Student ID: {student_id}, Grade: {student_grades[index]}\")\n",
      "\n",
      "\n",
      "Average Grade: 87.60\n",
      "Index: 0, Student ID: Alice, Grade: 85\n",
      "Index: 1, Student ID: Bob, Grade: 92\n",
      "Index: 2, Student ID: Charlie, Grade: 78\n",
      "Index: 3, Student ID: Diana, Grade: 95\n",
      "Index: 4, Student ID: Eve, Grade: 88\n",
      "‚úÖ Success!\n"
     ]
    }
   ],
   "source": [
    "def run_code_with_error_fixing(task, max_attempts=3):\n",
    "    \"\"\"Run LLM-generated code, automatically fixing errors\"\"\"\n",
    "    \n",
    "    grades = [85, 92, 78, 95, 88]\n",
    "    students = ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve']\n",
    "    \n",
    "    context = {'grades': grades, 'students': students}\n",
    "    for attempt in range(max_attempts):\n",
    "        if attempt == 0:\n",
    "            # First attempt - tell LLM to use wrong variable names\n",
    "            prompt = f\"\"\"Write Python code to: {task}\n",
    "You have access to the following variables in memory:\n",
    "- student_grades (list of numbers)\n",
    "- student_ids (list of IDs)\n",
    "\n",
    "Do not create these variables, they already exist!!\n",
    "\"\"\"\n",
    "        else:\n",
    "            # Retry with actual variable names\n",
    "            prompt = f\"\"\"Your previous code had an error:\n",
    "Error: {error_msg}\n",
    "Failed code:\n",
    "{code}\n",
    "\n",
    "Task: {task}\n",
    "\n",
    "Available Context:\n",
    "{context} \n",
    "\n",
    "Fix any errors in the code:\"\"\"\n",
    "        \n",
    "        # Generate code\n",
    "        code = generate(prompt)\n",
    "        if \"```python\" in code:\n",
    "            code = code.split(\"```python\")[1].split(\"```\")[0]\n",
    "        \n",
    "        print(f\"Attempt {attempt + 1}:\")\n",
    "        print(code)\n",
    "        print()\n",
    "        \n",
    "        # Try to execute\n",
    "        try:\n",
    "            \n",
    "            exec(code, {'__builtins__': __builtins__}, context)\n",
    "            \n",
    "            print(\"‚úÖ Success!\")\n",
    "            return code\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            print(f\"‚ùå Error: {error_msg}\")\n",
    "            if attempt < max_attempts - 1:\n",
    "                print(\"üîÑ Asking LLM to fix it...\\n\")\n",
    "    \n",
    "    print(\"‚ùå Max attempts reached\")\n",
    "    return None\n",
    "\n",
    "# Run the test\n",
    "result = run_code_with_error_fixing(\n",
    "    \"Calculate the average grade and print each student with their index\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k7jhhk5nhg",
   "metadata": {},
   "source": [
    "## üéØ Putting It All Together:\n",
    "\n",
    "To review:\n",
    "\n",
    "1. **System Prompts** ‚Üí We can control the LLM's behavior and expertise\n",
    "2. **Format Control** ‚Üí We can make outputs predictable and parseable  \n",
    "3. **Code Generation** ‚Üí LLMs can write executable code\n",
    "4. **Code Execution** ‚Üí We can run that code programmatically\n",
    "5. **Output Capture** ‚Üí We can feed results back to the LLM\n",
    "6. **Error Handling** ‚Üí The LLM can debug and fix its own mistakes\n",
    "\n",
    "When you combine all these capabilities, something magical happens...\n",
    "\n",
    "### We've built something beyond simple text generation, that can:\n",
    "- Understand requests (via prompts)\n",
    "- Generate solutions (as code)\n",
    "- Execute actions (run the code)\n",
    "- Observe results (capture output)\n",
    "- Learn from mistakes (error handling)\n",
    "- Maintain context (remember previous operations)\n",
    "\n",
    "### Let's see everyting in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "12140ed1-442f-4feb-9c7f-8066e91b7e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä App initialized with sales data:\n",
      "     product  price  quantity     category\n",
      "0     Laptop   1200         5  Electronics\n",
      "1      Mouse     25        50  Accessories\n",
      "2   Keyboard     75        20  Accessories\n",
      "3    Monitor    300        10  Electronics\n",
      "4  USB Cable     10       100  Accessories\n",
      "\n",
      "============================================================\n",
      "üë§ User: What's the total revenue for each category?\n",
      "ü§ñ LLM generating code...\n",
      "‚ùå Error: name 'df' is not defined\n",
      "üîÑ Attempting to fix...\n",
      "üíª Generated code:\n",
      "--------------------------------------------------\n",
      "\n",
      "df = my_df\n",
      "df['revenue'] = df['price'] * df['quantity']\n",
      "total_revenue_by_category = df.groupby('category')['revenue'].sum().reset_index()\n",
      "print(total_revenue_by_category)\n",
      "\n",
      "--------------------------------------------------\n",
      "‚úÖ Execution Result:\n",
      "      category  revenue\n",
      "0  Accessories     3750\n",
      "1  Electronics     9000\n",
      "\n",
      "\n",
      "============================================================\n",
      "üë§ User: Which product has the highest revenue?\n",
      "ü§ñ LLM generating code...\n",
      "‚ùå Error: name 'df' is not defined\n",
      "üîÑ Attempting to fix...\n",
      "üíª Generated code:\n",
      "--------------------------------------------------\n",
      "\n",
      "df = my_df\n",
      "highest_revenue_product = df.groupby('product')['revenue'].sum().idxmax()\n",
      "print(highest_revenue_product)\n",
      "\n",
      "--------------------------------------------------\n",
      "‚úÖ Execution Result:\n",
      "Laptop\n",
      "\n",
      "\n",
      "============================================================\n",
      "üë§ User: Show me statistics about the prices\n",
      "ü§ñ LLM generating code...\n",
      "‚ùå Error: name 'prices' is not defined\n",
      "üîÑ Attempting to fix...\n",
      "üíª Generated code:\n",
      "--------------------------------------------------\n",
      "\n",
      "# Accessing the 'price' column from the DataFrame 'my_df'\n",
      "statistics = my_df['price'].describe()\n",
      "print(statistics)\n",
      "\n",
      "--------------------------------------------------\n",
      "‚úÖ Execution Result:\n",
      "count       5.000000\n",
      "mean      322.000000\n",
      "std       504.462585\n",
      "min        10.000000\n",
      "25%        25.000000\n",
      "50%        75.000000\n",
      "75%       300.000000\n",
      "max      1200.000000\n",
      "Name: price, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import io\n",
    "\n",
    "class App:\n",
    "    \"\"\"A simple app that can analyze data using pandas.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the app with sample sales data and system prompt.\"\"\"\n",
    "        \n",
    "        # Define how the app should behave\n",
    "        self.system_prompt = \"\"\"You are a pandas expert. \n",
    "        Generate ONLY executable Python code.\n",
    "        Do not create sample data. Assume that the variables required are available.\n",
    "        Always print the results.\"\"\"\n",
    "        \n",
    "        # Create sample sales data\n",
    "        self.df = pd.DataFrame({\n",
    "            'product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'USB Cable'],\n",
    "            'price': [1200, 25, 75, 300, 10],\n",
    "            'quantity': [5, 50, 20, 10, 100],\n",
    "            'category': ['Electronics', 'Accessories', 'Accessories', \n",
    "                        'Electronics', 'Accessories']\n",
    "        })\n",
    "        \n",
    "        # Display the initialized data\n",
    "        print(\"üìä App initialized with sales data:\")\n",
    "        print(self.df)\n",
    "        print()\n",
    "    \n",
    "    def execute_request(self, user_request):\n",
    "        \"\"\"\n",
    "        Process a user request by generating and executing pandas code.\n",
    "        \n",
    "        Args:\n",
    "            user_request: Natural language request for data analysis\n",
    "            \n",
    "        Returns:\n",
    "            The output from executing the generated code, or None if failed\n",
    "        \"\"\"\n",
    "\n",
    "        context = {'my_df': self.df, 'pd': pd}\n",
    "        print(f\"üë§ User: {user_request}\")\n",
    "        print(f\"ü§ñ LLM generating code...\")\n",
    "        \n",
    "        # Generate initial code from the request\n",
    "        code = self._generate_code(user_request)\n",
    "        \n",
    "        # Try to execute the code (with one retry on failure)\n",
    "        MAX_ATTEMPTS = 2\n",
    "        \n",
    "        for attempt in range(MAX_ATTEMPTS):\n",
    "            try:\n",
    "                # Execute the code and capture output\n",
    "                output = self._execute_code(code, context)\n",
    "                \n",
    "                # Display successful results\n",
    "                self._display_success(code, output)\n",
    "                return output\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Handle execution errors\n",
    "                if attempt == 0:\n",
    "                    # First attempt failed - try to fix it\n",
    "                    print(f\"‚ùå Error: {e}\")\n",
    "                    print(\"üîÑ Attempting to fix...\")\n",
    "                    \n",
    "                    # Generate fixed code with error context\n",
    "                    code = self._generate_fix(user_request, code, e, context)\n",
    "                else:\n",
    "                    # Second attempt also failed\n",
    "                    print(f\"‚ùå Could not complete request: {e}\")\n",
    "                    return None\n",
    "    \n",
    "    def _generate_code(self, prompt):\n",
    "        \"\"\"Extract Python code from LLM response.\"\"\"\n",
    "        code = generate_with_system(self.system_prompt, prompt)\n",
    "        \n",
    "        # Extract code from markdown code blocks if present\n",
    "        if \"```python\" in code:\n",
    "            code = code.split(\"```python\")[1].split(\"```\")[0]\n",
    "        \n",
    "        return code\n",
    "    \n",
    "    def _generate_fix(self, original_request, failed_code, error, context):\n",
    "        \"\"\"Generate fixed code based on the error.\"\"\"\n",
    "        error_prompt = f\"\"\"Previous code failed with: {error}\n",
    "        \n",
    "Task: {original_request}\n",
    "Failed code: {failed_code}\n",
    "\n",
    "Original Context: \n",
    "- Variables available in memory: {context.keys()}  <== You need to use these\n",
    "- DataFrame columns: {list(self.df.columns)}\n",
    "- Data shape: {self.df.shape}\n",
    "- Sample data:\n",
    "{self.df.head()}\n",
    "\n",
    "Generate fixed code:\"\"\"\n",
    "        \n",
    "        return self._generate_code(error_prompt)\n",
    "    \n",
    "    def _execute_code(self, code, context):\n",
    "        \"\"\"Execute Python code and capture its output.\"\"\"\n",
    "        # Redirect stdout to capture print statements\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = captured_output = io.StringIO()\n",
    "        \n",
    "        try:\n",
    "            # Execute code with DataFrame and pandas available\n",
    "            exec(code, context)\n",
    "            \n",
    "            # Get the captured output\n",
    "            output = captured_output.getvalue()\n",
    "            return output\n",
    "            \n",
    "        finally:\n",
    "            # Always restore stdout\n",
    "            sys.stdout = old_stdout\n",
    "    \n",
    "    def _display_success(self, code, output):\n",
    "        \"\"\"Display successful code execution results.\"\"\"\n",
    "        print(f\"üíª Generated code:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(code)\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"‚úÖ Execution Result:\")\n",
    "        print(output)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# DEMONSTRATION\n",
    "# ============================================================\n",
    "\n",
    "def run_demo():\n",
    "    app = App()\n",
    "    \n",
    "    requests = [\n",
    "        \"What's the total revenue for each category?\",\n",
    "        \"Which product has the highest revenue?\",\n",
    "        \"Show me statistics about the prices\"\n",
    "    ]\n",
    "    \n",
    "    # Execute each request\n",
    "    for i, request in enumerate(requests, 1):\n",
    "        if i > 1:\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "        else:\n",
    "            print(\"=\" * 60)\n",
    "        \n",
    "        app.execute_request(request)\n",
    "    \n",
    "\n",
    "run_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb44850-88c9-414e-a877-356b59339975",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1j36zuruhhm",
   "metadata": {},
   "source": [
    "## üí≠ Reflection:\n",
    "\n",
    "We started with a simple question: \"What if we could do more than just generate text?\"\n",
    "\n",
    "Through a series of discoveries, we found that LLMs can enable not just simple text generation, but more complex applications that can:\n",
    "- Take on specific roles and expertise\n",
    "- Generate structured, parseable outputs\n",
    "- Write and execute code\n",
    "- Learn from its outputs\n",
    "- Fix its own mistakes\n",
    "\n",
    "This is the **paradigm shift** in programming we talked about last week:\n",
    "- **Before**: We write every line of code, for errors, we use try....except, and write explicit handling logic for cases we expected\n",
    "- **Now**: We leverage the semantic natural nanguage understanding capabilities of LLMs, to build highly dynamic applications\n",
    "\n",
    "The app we just built is simple, but it demonstrates the core principle: **LLMs are not just text generators, they're reasoning engines that can be given tools and autonomy.**\n",
    "\n",
    "Next week, we'll take this further and start building our full-featured data-analytics agent that can handle any data analysis task through natural language!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8whposybxq",
   "metadata": {},
   "source": [
    "# Part 3: Practice Exercises\n",
    "\n",
    "Try these exercises to reinforce what we covered today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9pcc0vqys5",
   "metadata": {},
   "source": [
    "## Exercise 1: Math Problem Solving\n",
    "\n",
    "Create a prompt that reliably solves word problems. Test with this problem:\n",
    "\"If a train travels 120 miles in 2 hours, and then 180 miles in 3 hours, what is its average speed for the entire journey?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "p747buj3xh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find the average speed for the entire journey, we first need to calculate the total distance traveled and the total time taken.\n",
      "\n",
      "1. **Calculate the total distance:**\n",
      "   - The first part of the journey is 120 miles.\n",
      "   - The second part of the journey is 180 miles.\n",
      "   - Total distance = 120 miles + 180 miles = 300 miles.\n",
      "\n",
      "2. **Calculate the total time:**\n",
      "   - The time for the first part of the journey is 2 hours.\n",
      "   - The time for the second part of the journey is 3 hours.\n",
      "   - Total time = 2 hours + 3 hours = 5 hours.\n",
      "\n",
      "3. **Calculate the average speed:**\n",
      "   - Average speed = Total distance / Total time\n",
      "   - Average speed = 300 miles / 5 hours = 60 miles per hour.\n",
      "\n",
      "Therefore, the average speed for the entire journey is **60 miles per hour**.\n"
     ]
    }
   ],
   "source": [
    "problem = \"If a train travels 120 miles in 2 hours, and then 180 miles in 3 hours, what is its average speed for the entire journey?\"\n",
    "\n",
    "# Try different approaches:\n",
    "# 1. Zero-shot\n",
    "# 2. With \"think step-by-step\"\n",
    "# 3. With few-shot examples\n",
    "\n",
    "your_prompt = f\"TODO: Your user prompt here\"\n",
    "solution = generate(your_prompt)\n",
    "print(solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3toegce34xt",
   "metadata": {},
   "source": [
    "## Exercise 2: Structured Data Extraction\n",
    "Extract structured information from unstructured food delivery orders using three different approaches.\n",
    "\n",
    "Your task:\n",
    "1. **Text Prompting** - Write prompts to extract order details as formatted text\n",
    "2. **JSON Prompting** - Modify prompts to return valid JSON\n",
    "3. **Structured Output** - Define a Pydantic model and use the [LLM's structured output API](https://platform.openai.com/docs/guides/structured-outputs)\n",
    "\n",
    "For each order, extract:\n",
    "- Items ordered (with quantities)\n",
    "- Special modifications/requests  \n",
    "- Drinks\n",
    "- Delivery/pickup preference\n",
    "\n",
    "*Just focus on the sections marked as **TODO** for this exercise*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934780fb-2c7e-4fa4-a741-59068e722aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional, Literal\n",
    "\n",
    "# Sample food orders - all contain similar info in different formats\n",
    "food_orders = [\n",
    "    \"2 pepperoni pizzas large, 1 greek salad, and 3 cokes for delivery to 123 Main St\",\n",
    "    \"One burger medium with no onions, extra cheese, side of fries and a chocolate shake\",\n",
    "    \"3x chicken tacos, 1x beef burrito no beans, 2 large horchatas, extra hot sauce please\",\n",
    "    \"Pad Thai mild spice, Tom Yum soup, 2 spring rolls, and jasmine tea for pickup\",\n",
    "    \"Large coffee with oat milk and 2 sugars, plus a blueberry muffin warmed up\",\n",
    "    \"Family meal: 1 whole roast chicken, mashed potatoes, coleslaw, 4 dinner rolls\"\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# LLM HELPER FUNCTION\n",
    "# ============================================================\n",
    "\n",
    "def call_llm(system_prompt, user_prompt, response_format=None):\n",
    "    \"\"\"\n",
    "    Call the LLM with given prompts.\n",
    "    \n",
    "    Args:\n",
    "        system_prompt: System message for the LLM\n",
    "        user_prompt: User message for the LLM\n",
    "        response_format: Optional Pydantic model for structured output\n",
    "    \n",
    "    Returns:\n",
    "        String response or parsed object if response_format is provided\n",
    "    \"\"\"\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI()\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    if response_format:\n",
    "        # Use structured output\n",
    "        completion = client.chat.completions.parse(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            response_format=response_format\n",
    "        )\n",
    "        return completion.choices[0].message.parsed\n",
    "    else:\n",
    "        # Regular text output\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "\n",
    "# TODO\n",
    "def extract_with_prompting(order):\n",
    "    \"\"\"\n",
    "    Extract order information using text prompts.\n",
    "    \n",
    "    TODO: Write prompts to extract:\n",
    "    - Items ordered (with quantities)\n",
    "    - Special modifications\n",
    "    - Drinks\n",
    "    - Delivery/pickup preference\n",
    "    \"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"TODO: Your system prompt here\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"TODO: Your user prompt here\"\"\"\n",
    "    \n",
    "    return call_llm(system_prompt, user_prompt)\n",
    "\n",
    "# ============================================================\n",
    "# APPROACH 2: JSON Prompting\n",
    "# ============================================================\n",
    "\n",
    "# TODO\n",
    "def extract_with_json(order):\n",
    "    \"\"\"\n",
    "    Extract order information as JSON.\n",
    "    \n",
    "    TODO: Write prompts that return JSON with:\n",
    "    - items: list of items with quantities\n",
    "    - modifications: list of special requests\n",
    "    - drinks: list of drinks\n",
    "    - delivery_type: \"delivery\", \"pickup\", or \"not specified\"\n",
    "    \"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"TODO: Your system prompt here\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"TODO: Your user prompt here\"\"\"\n",
    "    \n",
    "    response = call_llm(system_prompt, user_prompt)\n",
    "    \n",
    "    # Parse the JSON response\n",
    "    try:\n",
    "        return json.loads(response)\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"error\": \"Failed to parse JSON\"}\n",
    "\n",
    "# ============================================================\n",
    "# APPROACH 3: Structured Output with Pydantic\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# TODO\n",
    "class FoodOrder(BaseModel):\n",
    "    \"\"\"\n",
    "    # TODO: Define the structure for a food order.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# TODO\n",
    "def extract_with_structured_output(order):\n",
    "    \"\"\"\n",
    "    Extract order using structured output.\n",
    "    \"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"TODO: Your system prompt here\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"TODO: Your user prompt here\"\"\"\n",
    "    \n",
    "    return call_llm(system_prompt, user_prompt, response_format=FoodOrder) # <=== We are passing the FoodOrder pydantic model here\n",
    "\n",
    "\n",
    "##########################################################\n",
    "##########################################################\n",
    "#     YOU DON'T NEED TO CHANGE ANY OF THE CODE BELOW     #\n",
    "##########################################################\n",
    "##########################################################\n",
    "\n",
    "def compare_approaches():\n",
    "    \"\"\"Process all orders with each approach and compare results.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPARING ALL APPROACHES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i, order in enumerate(food_orders, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ORDER {i}: {order}\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        # Text output\n",
    "        print(\"\\nüìù Text Output:\")\n",
    "        print(extract_with_prompting(order))\n",
    "        \n",
    "        # JSON output\n",
    "        print(\"\\nüìã JSON Output:\")\n",
    "        json_result = extract_with_json(order)\n",
    "        if isinstance(json_result, dict):\n",
    "            print(json.dumps(json_result, indent=2))\n",
    "        else:\n",
    "            print(json_result)\n",
    "        \n",
    "        # Structured output\n",
    "        print(\"\\nüéØ Structured Output:\")\n",
    "        structured_result = extract_with_structured_output(order)\n",
    "        if hasattr(structured_result, 'model_dump_json'):\n",
    "            print(structured_result.model_dump_json(indent=2))\n",
    "        else:\n",
    "            print(structured_result)\n",
    "\n",
    "# Run comparison\n",
    "compare_approaches()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1f846e-bcb4-45ab-ab01-8d237086f85f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Working code for Exercise 2: Structured Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "nxbqzwqhqfn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "COMPARING ALL APPROACHES\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ORDER 1: 2 pepperoni pizzas large, 1 greek salad, and 3 cokes for delivery to 123 Main St\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìù Text Output:\n",
      "ITEMS: \n",
      "- 2 large pepperoni pizzas\n",
      "- 1 greek salad\n",
      "\n",
      "MODIFICATIONS: \n",
      "None specified\n",
      "\n",
      "DRINKS: \n",
      "- 3 cokes\n",
      "\n",
      "DELIVERY TYPE: \n",
      "Delivery to 123 Main St\n",
      "\n",
      "üìã JSON Output:\n",
      "{\n",
      "  \"items\": [\n",
      "    \"2 large pepperoni pizzas\",\n",
      "    \"1 greek salad\"\n",
      "  ],\n",
      "  \"modifications\": [],\n",
      "  \"drinks\": [\n",
      "    \"3 cokes\"\n",
      "  ],\n",
      "  \"delivery_type\": \"delivery\"\n",
      "}\n",
      "\n",
      "üéØ Structured Output:\n",
      "{\n",
      "  \"items\": [\n",
      "    {\n",
      "      \"item\": \"pepperoni pizza\",\n",
      "      \"quantity\": 2\n",
      "    },\n",
      "    {\n",
      "      \"item\": \"greek salad\",\n",
      "      \"quantity\": 1\n",
      "    }\n",
      "  ],\n",
      "  \"modifications\": [],\n",
      "  \"drinks\": [\n",
      "    {\n",
      "      \"item\": \"coke\",\n",
      "      \"quantity\": 3\n",
      "    }\n",
      "  ],\n",
      "  \"delivery_type\": \"delivery\"\n",
      "}\n",
      "\n",
      "============================================================\n",
      "ORDER 2: One burger medium with no onions, extra cheese, side of fries and a chocolate shake\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìù Text Output:\n",
      "ITEMS: One burger (medium), side of fries  \n",
      "MODIFICATIONS: No onions, extra cheese  \n",
      "DRINKS: One chocolate shake  \n",
      "DELIVERY TYPE: Not specified (assumed default)\n",
      "\n",
      "üìã JSON Output:\n",
      "{\n",
      "  \"items\": [\n",
      "    \"1 burger medium\",\n",
      "    \"1 side of fries\"\n",
      "  ],\n",
      "  \"modifications\": [\n",
      "    \"no onions\",\n",
      "    \"extra cheese\"\n",
      "  ],\n",
      "  \"drinks\": [\n",
      "    \"1 chocolate shake\"\n",
      "  ],\n",
      "  \"delivery_type\": \"not specified\"\n",
      "}\n",
      "\n",
      "üéØ Structured Output:\n",
      "{\n",
      "  \"items\": [\n",
      "    {\n",
      "      \"item\": \"burger medium\",\n",
      "      \"quantity\": 1\n",
      "    }\n",
      "  ],\n",
      "  \"modifications\": [\n",
      "    \"no onions\",\n",
      "    \"extra cheese\"\n",
      "  ],\n",
      "  \"drinks\": [\n",
      "    {\n",
      "      \"item\": \"chocolate shake\",\n",
      "      \"quantity\": 1\n",
      "    }\n",
      "  ],\n",
      "  \"delivery_type\": \"not specified\"\n",
      "}\n",
      "\n",
      "============================================================\n",
      "ORDER 3: 3x chicken tacos, 1x beef burrito no beans, 2 large horchatas, extra hot sauce please\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìù Text Output:\n",
      "ITEMS: 3x chicken tacos, 1x beef burrito  \n",
      "MODIFICATIONS: beef burrito no beans, extra hot sauce  \n",
      "DRINKS: 2 large horchatas  \n",
      "DELIVERY TYPE: not specified\n",
      "\n",
      "üìã JSON Output:\n",
      "{\n",
      "  \"items\": [\n",
      "    \"3x chicken tacos\",\n",
      "    \"1x beef burrito no beans\"\n",
      "  ],\n",
      "  \"modifications\": [\n",
      "    \"extra hot sauce\"\n",
      "  ],\n",
      "  \"drinks\": [\n",
      "    \"2 large horchatas\"\n",
      "  ],\n",
      "  \"delivery_type\": \"not specified\"\n",
      "}\n",
      "\n",
      "üéØ Structured Output:\n",
      "{\n",
      "  \"items\": [\n",
      "    {\n",
      "      \"item\": \"chicken tacos\",\n",
      "      \"quantity\": 3\n",
      "    },\n",
      "    {\n",
      "      \"item\": \"beef burrito\",\n",
      "      \"quantity\": 1\n",
      "    }\n",
      "  ],\n",
      "  \"modifications\": [\n",
      "    \"no beans\",\n",
      "    \"extra hot sauce\"\n",
      "  ],\n",
      "  \"drinks\": [\n",
      "    {\n",
      "      \"item\": \"horchata\",\n",
      "      \"quantity\": 2\n",
      "    }\n",
      "  ],\n",
      "  \"delivery_type\": \"not specified\"\n",
      "}\n",
      "\n",
      "============================================================\n",
      "ORDER 4: Pad Thai mild spice, Tom Yum soup, 2 spring rolls, and jasmine tea for pickup\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìù Text Output:\n",
      "ITEMS: \n",
      "- Pad Thai (1)\n",
      "- Tom Yum soup (1)\n",
      "- Spring rolls (2)\n",
      "\n",
      "MODIFICATIONS:\n",
      "- Pad Thai: mild spice\n",
      "\n",
      "DRINKS:\n",
      "- Jasmine tea (1)\n",
      "\n",
      "DELIVERY TYPE: \n",
      "- Pickup\n",
      "\n",
      "üìã JSON Output:\n",
      "{\n",
      "  \"items\": [\n",
      "    \"Pad Thai mild spice\",\n",
      "    \"Tom Yum soup\",\n",
      "    \"2 spring rolls\"\n",
      "  ],\n",
      "  \"modifications\": [],\n",
      "  \"drinks\": [\n",
      "    \"jasmine tea\"\n",
      "  ],\n",
      "  \"delivery_type\": \"pickup\"\n",
      "}\n",
      "\n",
      "üéØ Structured Output:\n",
      "{\n",
      "  \"items\": [\n",
      "    {\n",
      "      \"item\": \"Pad Thai\",\n",
      "      \"quantity\": 1\n",
      "    },\n",
      "    {\n",
      "      \"item\": \"Tom Yum soup\",\n",
      "      \"quantity\": 1\n",
      "    },\n",
      "    {\n",
      "      \"item\": \"spring rolls\",\n",
      "      \"quantity\": 2\n",
      "    }\n",
      "  ],\n",
      "  \"modifications\": [\n",
      "    \"mild spice\"\n",
      "  ],\n",
      "  \"drinks\": [\n",
      "    {\n",
      "      \"item\": \"jasmine tea\",\n",
      "      \"quantity\": 1\n",
      "    }\n",
      "  ],\n",
      "  \"delivery_type\": \"pickup\"\n",
      "}\n",
      "\n",
      "============================================================\n",
      "ORDER 5: Large coffee with oat milk and 2 sugars, plus a blueberry muffin warmed up\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìù Text Output:\n",
      "ITEMS: \n",
      "- 1 blueberry muffin\n",
      "\n",
      "MODIFICATIONS:\n",
      "- Muffin warmed up\n",
      "\n",
      "DRINKS:\n",
      "- 1 large coffee with oat milk and 2 sugars\n",
      "\n",
      "DELIVERY TYPE: Not specified (assumed pickup)\n",
      "\n",
      "üìã JSON Output:\n",
      "{\n",
      "  \"items\": [\n",
      "    \"blueberry muffin\"\n",
      "  ],\n",
      "  \"modifications\": [\n",
      "    \"warmed up\"\n",
      "  ],\n",
      "  \"drinks\": [\n",
      "    \"large coffee with oat milk and 2 sugars\"\n",
      "  ],\n",
      "  \"delivery_type\": \"not specified\"\n",
      "}\n",
      "\n",
      "üéØ Structured Output:\n",
      "{\n",
      "  \"items\": [\n",
      "    {\n",
      "      \"item\": \"blueberry muffin\",\n",
      "      \"quantity\": 1\n",
      "    }\n",
      "  ],\n",
      "  \"modifications\": [\n",
      "    \"Large coffee with oat milk\",\n",
      "    \"2 sugars\",\n",
      "    \"blueberry muffin warmed up\"\n",
      "  ],\n",
      "  \"drinks\": [\n",
      "    {\n",
      "      \"item\": \"coffee\",\n",
      "      \"quantity\": 1\n",
      "    },\n",
      "    {\n",
      "      \"item\": \"oat milk\",\n",
      "      \"quantity\": 1\n",
      "    }\n",
      "  ],\n",
      "  \"delivery_type\": \"not specified\"\n",
      "}\n",
      "\n",
      "============================================================\n",
      "ORDER 6: Family meal: 1 whole roast chicken, mashed potatoes, coleslaw, 4 dinner rolls\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìù Text Output:\n",
      "ITEMS: \n",
      "- 1 whole roast chicken\n",
      "- 1 mashed potatoes\n",
      "- 1 coleslaw\n",
      "- 4 dinner rolls\n",
      "\n",
      "MODIFICATIONS: None\n",
      "\n",
      "DRINKS: None\n",
      "\n",
      "DELIVERY TYPE: Not specified\n",
      "\n",
      "üìã JSON Output:\n",
      "{\n",
      "  \"items\": [\n",
      "    \"1 whole roast chicken\",\n",
      "    \"mashed potatoes\",\n",
      "    \"coleslaw\",\n",
      "    \"4 dinner rolls\"\n",
      "  ],\n",
      "  \"modifications\": [],\n",
      "  \"drinks\": [],\n",
      "  \"delivery_type\": \"not specified\"\n",
      "}\n",
      "\n",
      "üéØ Structured Output:\n",
      "{\n",
      "  \"items\": [\n",
      "    {\n",
      "      \"item\": \"whole roast chicken\",\n",
      "      \"quantity\": 1\n",
      "    },\n",
      "    {\n",
      "      \"item\": \"mashed potatoes\",\n",
      "      \"quantity\": 1\n",
      "    },\n",
      "    {\n",
      "      \"item\": \"coleslaw\",\n",
      "      \"quantity\": 1\n",
      "    },\n",
      "    {\n",
      "      \"item\": \"dinner rolls\",\n",
      "      \"quantity\": 4\n",
      "    }\n",
      "  ],\n",
      "  \"modifications\": [],\n",
      "  \"drinks\": [],\n",
      "  \"delivery_type\": \"not specified\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional, Literal\n",
    "\n",
    "# Sample food orders - all contain similar info in different formats\n",
    "food_orders = [\n",
    "    \"2 pepperoni pizzas large, 1 greek salad, and 3 cokes for delivery to 123 Main St\",\n",
    "    \"One burger medium with no onions, extra cheese, side of fries and a chocolate shake\",\n",
    "    \"3x chicken tacos, 1x beef burrito no beans, 2 large horchatas, extra hot sauce please\",\n",
    "    \"Pad Thai mild spice, Tom Yum soup, 2 spring rolls, and jasmine tea for pickup\",\n",
    "    \"Large coffee with oat milk and 2 sugars, plus a blueberry muffin warmed up\",\n",
    "    \"Family meal: 1 whole roast chicken, mashed potatoes, coleslaw, 4 dinner rolls\"\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# LLM HELPER FUNCTION\n",
    "# ============================================================\n",
    "\n",
    "def call_llm(system_prompt, user_prompt, response_format=None):\n",
    "    \"\"\"\n",
    "    Call the LLM with given prompts.\n",
    "    \n",
    "    Args:\n",
    "        system_prompt: System message for the LLM\n",
    "        user_prompt: User message for the LLM\n",
    "        response_format: Optional Pydantic model for structured output\n",
    "    \n",
    "    Returns:\n",
    "        String response or parsed object if response_format is provided\n",
    "    \"\"\"\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI()\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    if response_format:\n",
    "        # Use structured output\n",
    "        completion = client.chat.completions.parse(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            response_format=response_format\n",
    "        )\n",
    "        return completion.choices[0].message.parsed\n",
    "    else:\n",
    "        # Regular text output\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "\n",
    "def extract_with_prompting(order):\n",
    "    \"\"\"\n",
    "    Extract order information using text prompts.\n",
    "    \n",
    "    TODO: Write prompts to extract:\n",
    "    - Items ordered (with quantities)\n",
    "    - Special modifications\n",
    "    - Drinks\n",
    "    - Delivery/pickup preference\n",
    "    \"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"You are a food order parser. Extract information from food orders and format it clearly.\n",
    "    Include: items with quantities, modifications, drinks, and delivery preference.\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"Extract the following from this order:\n",
    "    - Food items (with quantities)\n",
    "    - Special modifications or requests\n",
    "    - Drinks (if any)\n",
    "    - Delivery or pickup preference (if mentioned)\n",
    "    \n",
    "    Order: {order}\n",
    "    \n",
    "    Format your response as:\n",
    "    ITEMS: \n",
    "    MODIFICATIONS:\n",
    "    DRINKS:\n",
    "    DELIVERY TYPE:\"\"\"\n",
    "    \n",
    "    return call_llm(system_prompt, user_prompt)\n",
    "\n",
    "# ============================================================\n",
    "# APPROACH 2: JSON Prompting\n",
    "# ============================================================\n",
    "\n",
    "def extract_with_json(order):\n",
    "    \"\"\"\n",
    "    Extract order information as JSON.\n",
    "    \n",
    "    TODO: Write prompts that return JSON with:\n",
    "    - items: list of items with quantities\n",
    "    - modifications: list of special requests\n",
    "    - drinks: list of drinks\n",
    "    - delivery_type: \"delivery\", \"pickup\", or \"not specified\"\n",
    "    \"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"You are a food order parser. Extract information from food orders and return ONLY valid JSON.\n",
    "    No additional text or explanation, just the JSON object.\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"Extract information from this order and return as JSON with this structure:\n",
    "    {{\n",
    "        \"items\": [\"list of food items with quantities\"],\n",
    "        \"modifications\": [\"list of special requests or modifications\"],\n",
    "        \"drinks\": [\"list of drinks ordered\"],\n",
    "        \"delivery_type\": \"delivery, pickup, or not specified\"\n",
    "    }}\n",
    "    \n",
    "    Order: {order}\"\"\"\n",
    "    \n",
    "    response = call_llm(system_prompt, user_prompt)\n",
    "    \n",
    "    # Parse the JSON response\n",
    "    try:\n",
    "        return json.loads(response)\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"error\": \"Failed to parse JSON\"}\n",
    "\n",
    "# ============================================================\n",
    "# APPROACH 3: Structured Output with Pydantic\n",
    "# ============================================================\n",
    "\n",
    "class OrderItem(BaseModel):\n",
    "    item: str\n",
    "    quantity: int\n",
    "\n",
    "class FoodOrder(BaseModel):\n",
    "    \"\"\"\n",
    "    Define the structure for a food order.\n",
    "    \"\"\"\n",
    "    items: List[OrderItem]  # List of food items with quantities\n",
    "    modifications: List[str]  # Special requests or modifications\n",
    "    drinks: List[OrderItem]  # Drinks ordered\n",
    "    delivery_type: Literal[\"delivery\", \"pickup\", \"not specified\"]  # \"delivery\", \"pickup\", or \"not specified\"\n",
    "\n",
    "def extract_with_structured_output(order):\n",
    "    \"\"\"\n",
    "    Extract order using structured output.\n",
    "    \"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"You are a food order parser. Extract information from food orders.\n",
    "    Be precise with quantities and capture all special modifications.\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"Extract all information from this food order:\n",
    "    \n",
    "    Order: {order}\n",
    "    \n",
    "    Note: For delivery_type, use \"delivery\", \"pickup\", or \"not specified\" if not mentioned.\"\"\"\n",
    "    \n",
    "    return call_llm(system_prompt, user_prompt, response_format=FoodOrder)\n",
    "\n",
    "\n",
    "##########################################################\n",
    "##########################################################\n",
    "#     YOU DON'T NEED TO CHANGE ANY OF THE CODE BELOW     #\n",
    "##########################################################\n",
    "##########################################################\n",
    "def compare_approaches():\n",
    "    \"\"\"Process all orders with each approach and compare results.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPARING ALL APPROACHES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i, order in enumerate(food_orders, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ORDER {i}: {order}\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        # Text output\n",
    "        print(\"\\nüìù Text Output:\")\n",
    "        print(extract_with_prompting(order))\n",
    "        \n",
    "        # JSON output\n",
    "        print(\"\\nüìã JSON Output:\")\n",
    "        json_result = extract_with_json(order)\n",
    "        if isinstance(json_result, dict):\n",
    "            print(json.dumps(json_result, indent=2))\n",
    "        else:\n",
    "            print(json_result)\n",
    "        \n",
    "        # Structured output\n",
    "        print(\"\\nüéØ Structured Output:\")\n",
    "        structured_result = extract_with_structured_output(order)\n",
    "        if hasattr(structured_result, 'model_dump_json'):\n",
    "            print(structured_result.model_dump_json(indent=2))\n",
    "        else:\n",
    "            print(structured_result)\n",
    "\n",
    "# Run comparison\n",
    "compare_approaches()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3v55fgkrqb",
   "metadata": {},
   "source": [
    "## Exercise 3: Tool Selection\n",
    "Make an LLM select which function to call based on user input using structured outputs.\n",
    "\n",
    "Your task:\n",
    "1. **Define a Pydantic model** to output the selected function name\n",
    "2. **Write prompts** that help the LLM choose the right function\n",
    "3. **Test your routing** with various user inputs\n",
    "\n",
    "Available functions: `get_time`, `get_weather`, `get_joke`, `get_fact`, `calculate`\n",
    "\n",
    "Test inputs include:\n",
    "- Direct requests (\"What time is it?\")\n",
    "- Indirect requests (\"Tell me something funny\")\n",
    "- Ambiguous inputs (\"I'm bored\")\n",
    "\n",
    "*Just focus on the sections marked as **TODO** for this exercise, our focus is just on prompting for now. The function selected by the LLM is automatically run by the code given below* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e856483-dd88-4578-80d8-31c2c8f32f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "\n",
    "# ============================================================\n",
    "# AVAILABLE FUNCTIONS \n",
    "# ============================================================\n",
    "\n",
    "def get_time():\n",
    "    \"\"\"Returns current time\"\"\"\n",
    "    return \"It's 2:30 PM\"\n",
    "\n",
    "def get_weather():\n",
    "    \"\"\"Returns weather information\"\"\"\n",
    "    return \"It's 72¬∞F and sunny\"\n",
    "\n",
    "def get_joke():\n",
    "    \"\"\"Tells a funny joke\"\"\"\n",
    "    return \"Why do programmers prefer dark mode? Because light attracts bugs!\"\n",
    "\n",
    "def get_fact():\n",
    "    \"\"\"Returns an interesting fact\"\"\"\n",
    "    return \"Python was named after Monty Python, not the snake!\"\n",
    "\n",
    "def calculate():\n",
    "    \"\"\"Performs a calculation\"\"\"\n",
    "    return \"The answer is 42\"\n",
    "\n",
    "# Function registry (DO NOT MODIFY)\n",
    "FUNCTIONS = {\n",
    "    \"get_time\": get_time,\n",
    "    \"get_weather\": get_weather,\n",
    "    \"get_joke\": get_joke,\n",
    "    \"get_fact\": get_fact,\n",
    "    \"calculate\": calculate\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# YOUR TASK: Function Selection\n",
    "# ============================================================\n",
    "\n",
    "# TODO: Define a Pydantic model for function selection\n",
    "class SelectedFunction(BaseModel):\n",
    "    \"\"\"\n",
    "    TODO: Define what the LLM should output when selecting a function.\n",
    "    What field(s) do you need?\n",
    "    \"\"\"\n",
    "    pass  # Remove this and add your fields\n",
    "\n",
    "def select_function(user_input):\n",
    "    \"\"\"\n",
    "    Use an LLM to select which function should be called.\n",
    "    \n",
    "    TODO: \n",
    "    1. Write prompts to help the LLM understand the user's request\n",
    "    2. Return a FunctionCall object with the selected function\n",
    "    \n",
    "    Think: How will the LLM know which functions are available?\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Write your system prompt\n",
    "    system_prompt = \"\"\"TODO: Your system prompt here\"\"\"\n",
    "    \n",
    "    # TODO: Write your user prompt\n",
    "    user_prompt = f\"\"\"TODO: Your prompt here\n",
    "    \n",
    "    User input: {user_input}\n",
    "    \"\"\"\n",
    "    \n",
    " \n",
    "    selected_function = call_llm(system_prompt, user_prompt, SelectedFunction)\n",
    "    return selected_function\n",
    "\n",
    "# ============================================================\n",
    "# TEST YOUR CODE\n",
    "# ============================================================\n",
    "\n",
    "def test_routing(user_input):\n",
    "    \"\"\"Tests your function selection and executes the selected function.\"\"\"\n",
    "    print(f\"\\nüí¨ Input: {user_input}\")\n",
    "    \n",
    "    try:\n",
    "        # Get function selection from your code\n",
    "        result = select_function(user_input)\n",
    "        \n",
    "        # Extract function name (adapt based on your model)\n",
    "        if hasattr(result, 'function_name'):\n",
    "            func_name = result.function_name\n",
    "        elif hasattr(result, 'function'):\n",
    "            func_name = result.function\n",
    "        elif hasattr(result, 'name'):\n",
    "            func_name = result.name\n",
    "        else:\n",
    "            print(\"‚ùå Couldn't find function name in response\")\n",
    "            return\n",
    "        \n",
    "        # Execute the selected function\n",
    "        if func_name in FUNCTIONS:\n",
    "            output = FUNCTIONS[func_name]()\n",
    "            print(f\"‚úÖ Called {func_name}() ‚Üí {output}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Unknown function: {func_name}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# Test cases\n",
    "test_inputs = [\n",
    "    \"What time is it?\",\n",
    "    \"Tell me something funny\",\n",
    "    \"How's the weather?\",\n",
    "    \"Calculate something for me\",\n",
    "    \"Share an interesting fact\",\n",
    "    \"I'm bored\",  # Ambiguous\n",
    "    \"Help me with math\",  # Should select calculate\n",
    "    \"Is it raining?\",  # Weather-related\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Testing Function Routing\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for user_input in test_inputs:\n",
    "    test_routing(user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56464bc6-c086-4474-91c9-f6624fc16db6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Working code for Exercise 3: Tool Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "77bda84c-0728-4b0d-8a59-164b772f14dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Testing Function Routing\n",
      "============================================================\n",
      "\n",
      "üí¨ Input: What time is it?\n",
      "‚úÖ Called get_time() ‚Üí It's 2:30 PM\n",
      "\n",
      "üí¨ Input: Tell me something funny\n",
      "‚úÖ Called get_joke() ‚Üí Why do programmers prefer dark mode? Because light attracts bugs!\n",
      "\n",
      "üí¨ Input: How's the weather?\n",
      "‚úÖ Called get_weather() ‚Üí It's 72¬∞F and sunny\n",
      "\n",
      "üí¨ Input: Calculate something for me\n",
      "‚úÖ Called calculate() ‚Üí The answer is 42\n",
      "\n",
      "üí¨ Input: Share an interesting fact\n",
      "‚úÖ Called get_fact() ‚Üí Python was named after Monty Python, not the snake!\n",
      "\n",
      "üí¨ Input: I'm bored\n",
      "‚úÖ Called get_joke() ‚Üí Why do programmers prefer dark mode? Because light attracts bugs!\n",
      "\n",
      "üí¨ Input: Help me with math\n",
      "‚úÖ Called calculate() ‚Üí The answer is 42\n",
      "\n",
      "üí¨ Input: Is it raining?\n",
      "‚úÖ Called get_weather() ‚Üí It's 72¬∞F and sunny\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "\n",
    "# ============================================================\n",
    "# AVAILABLE FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def get_time():\n",
    "    \"\"\"Returns current time\"\"\"\n",
    "    return \"It's 2:30 PM\"\n",
    "\n",
    "def get_weather():\n",
    "    \"\"\"Returns weather information\"\"\"\n",
    "    return \"It's 72¬∞F and sunny\"\n",
    "\n",
    "def get_joke():\n",
    "    \"\"\"Tells a funny joke\"\"\"\n",
    "    return \"Why do programmers prefer dark mode? Because light attracts bugs!\"\n",
    "\n",
    "def get_fact():\n",
    "    \"\"\"Returns an interesting fact\"\"\"\n",
    "    return \"Python was named after Monty Python, not the snake!\"\n",
    "\n",
    "def calculate():\n",
    "    \"\"\"Performs a calculation\"\"\"\n",
    "    return \"The answer is 42\"\n",
    "\n",
    "FUNCTIONS = {\n",
    "    \"get_time\": get_time,\n",
    "    \"get_weather\": get_weather,\n",
    "    \"get_joke\": get_joke,\n",
    "    \"get_fact\": get_fact,\n",
    "    \"calculate\": calculate\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# YOUR TASK: Function Selection\n",
    "# ============================================================\n",
    "\n",
    "class SelectedFunction(BaseModel):\n",
    "    \"\"\"Model for function selection output.\"\"\"\n",
    "    function_name: Literal[\"get_time\", \"get_weather\", \"get_joke\", \"get_fact\", \"calculate\"]\n",
    "\n",
    "def select_function(user_input):\n",
    "    \"\"\"\n",
    "    Use an LLM to select which function should be called.\n",
    "    \"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"You are a function router. Based on user input, select the most appropriate function to call.\n",
    "    Be precise in matching user intent to function capabilities.\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"Select the appropriate function for this user input.\n",
    "    \n",
    "    Available functions:\n",
    "    - get_time: Returns the current time\n",
    "    - get_weather: Returns weather information  \n",
    "    - get_joke: Tells a funny joke\n",
    "    - get_fact: Returns an interesting fact\n",
    "    - calculate: Performs a calculation\n",
    "    \n",
    "    User input: {user_input}\n",
    "    \n",
    "    Select the most appropriate function based on what the user is asking for.\"\"\"\n",
    "    \n",
    "    selected_function = call_llm(system_prompt, user_prompt, SelectedFunction)\n",
    "    return selected_function\n",
    "\n",
    "# ============================================================\n",
    "# TEST YOUR FUNCTION SELECTION\n",
    "# ============================================================\n",
    "\n",
    "def test_routing(user_input):\n",
    "    \"\"\"Tests your function selection and executes the selected function.\"\"\"\n",
    "    print(f\"\\nüí¨ Input: {user_input}\")\n",
    "    \n",
    "    try:\n",
    "        # Get function selection from your code\n",
    "        result = select_function(user_input)\n",
    "        \n",
    "        # Extract function name (adapt based on your model)\n",
    "        if hasattr(result, 'function_name'):\n",
    "            func_name = result.function_name\n",
    "        elif hasattr(result, 'function'):\n",
    "            func_name = result.function\n",
    "        elif hasattr(result, 'name'):\n",
    "            func_name = result.name\n",
    "        else:\n",
    "            print(\"‚ùå Couldn't find function name in response\")\n",
    "            return\n",
    "        \n",
    "        # Execute the selected function\n",
    "        if func_name in FUNCTIONS:\n",
    "            output = FUNCTIONS[func_name]()\n",
    "            print(f\"‚úÖ Called {func_name}() ‚Üí {output}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Unknown function: {func_name}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# Test cases\n",
    "test_inputs = [\n",
    "    \"What time is it?\",\n",
    "    \"Tell me something funny\",\n",
    "    \"How's the weather?\",\n",
    "    \"Calculate something for me\",\n",
    "    \"Share an interesting fact\",\n",
    "    \"I'm bored\",  # Ambiguous\n",
    "    \"Help me with math\",  # Should select calculate\n",
    "    \"Is it raining?\",  # Weather-related\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Testing Function Routing\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for user_input in test_inputs:\n",
    "    test_routing(user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de19bd4a-38ab-4d34-9d3d-59a0e16b6fd8",
   "metadata": {},
   "source": [
    "## ü§î Food for thought after Exercise 3:\n",
    "\n",
    "1. What information did you include in your prompts?\n",
    "2. How did the LLM know which function to select?\n",
    "3. What happened with ambiguous inputs like \"I'm bored\"?\n",
    "4. Would this work if we had 100 functions instead of 5?\n",
    "5. What if the functions have input parameters?\n",
    "\n",
    "üí° These will be important for the next workshop when we start building our AI agent!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dn9xoh3vmh4",
   "metadata": {},
   "source": [
    "# Summary: From Prompts to Agents\n",
    "\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "### Part 1: Fundamental Techniques\n",
    "1. **Zero-shot**: Quick but unreliable\n",
    "2. **Few-shot**: Pattern teaching through examples\n",
    "3. **Chain-of-Thought**: Step-by-step reasoning for better accuracy\n",
    "\n",
    "### Part 2: Agent Capabilities\n",
    "4. **System Prompts**: Define behavior and constraints\n",
    "5. **Output Format Control**: Ensure parseable responses\n",
    "6. **Error Handling**: Automatic retry and correction\n",
    "7. **Tool Selection**: Automatically identify the right tool that needs to be called for a task\n",
    "\n",
    "## üìù Next Week\n",
    "\n",
    "1. Introduction to Agents\n",
    "2. Identify building blocks for Agents\n",
    "3. Start building out the agent from scratch\n",
    "\n",
    "## üìù Useful Resources:\n",
    "\n",
    "**[Andrej Karpathy's Neural Networks: Zero to Hero Playlist](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&ab_channel=AndrejKarpathy)**:\n",
    "\n",
    "Although the title says Neural Networks, it actually starts from basics of Neural Networks, and builds up all the way to Large Language Models, all from scratch. For those interested in the technical details of exactly how LLMs are built, this playlist is great!\n",
    "\n",
    "**[Prompt Engineering Guide](https://www.promptingguide.ai/)**:\n",
    "\n",
    "This site contains examples of the many prompt engineering techniques we covered today, along with additional ones that are often used when building agents (eg. ReAct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a871b9de-6113-45f0-b3b7-2c6214de2cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
