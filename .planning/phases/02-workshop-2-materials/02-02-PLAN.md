---
phase: 02-workshop-2-materials
plan: 02
type: execute
wave: 2
depends_on: [02-01]
files_modified:
  - demos/src/demos/llm-intuition/components/LiveMode.tsx
  - demos/src/demos/llm-intuition/components/TokenDisplay.tsx
  - demos/src/demos/llm-intuition/components/SystemPromptPicker.tsx
  - demos/src/demos/llm-intuition/store/useLiveModeStore.ts
  - demos/src/demos/llm-intuition/data/simulated-live-response.ts
  - demos/src/demos/llm-intuition/index.tsx
  - demos/src/services/api.ts
autonomous: true
requirements: [INFR-02, DEMO-01, DEMO-02]

must_haves:
  truths:
    - "Live tab shows token-by-token generation with probability bars for each token's top 5 alternatives"
    - "User can select from system prompt presets (None, Helpful Assistant, Pirate, Data Scientist, JSON Only, Custom) and see how probabilities change"
    - "User can click any completed token to see its alternatives and pick a different one, triggering re-generation from that point"
    - "Live mode works in simulated mode (no API key) using pre-recorded token probability data"
    - "Live mode works with real OpenAI API when instructor activates hidden 'live' toggle"
    - "The core aha moment is visible: changing system prompt reshapes probability distribution at every token position"
  artifacts:
    - path: "demos/src/demos/llm-intuition/components/LiveMode.tsx"
      provides: "Main live mode container with prompt input, generate button, and token stream"
      min_lines: 80
    - path: "demos/src/demos/llm-intuition/components/TokenDisplay.tsx"
      provides: "Clickable token stream with probability bar expansion for selected token"
      min_lines: 60
    - path: "demos/src/demos/llm-intuition/components/SystemPromptPicker.tsx"
      provides: "System prompt preset selector with custom text field"
      min_lines: 40
    - path: "demos/src/demos/llm-intuition/store/useLiveModeStore.ts"
      provides: "Zustand store for live mode state: tokens, streaming status, system prompt, selected token"
      min_lines: 80
    - path: "demos/src/services/api.ts"
      provides: "streamWithLogprobs function for OpenAI streaming with token probabilities"
      contains: "streamWithLogprobs"
    - path: "demos/src/demos/llm-intuition/data/simulated-live-response.ts"
      provides: "Pre-recorded token probability data for 2-3 scenarios"
      min_lines: 50
  key_links:
    - from: "demos/src/demos/llm-intuition/components/LiveMode.tsx"
      to: "demos/src/services/api.ts"
      via: "streamWithLogprobs call for real API mode"
      pattern: "streamWithLogprobs"
    - from: "demos/src/demos/llm-intuition/components/LiveMode.tsx"
      to: "demos/src/demos/llm-intuition/store/useLiveModeStore.ts"
      via: "Zustand store for token state"
      pattern: "useLiveModeStore"
    - from: "demos/src/demos/llm-intuition/components/TokenDisplay.tsx"
      to: "demos/src/demos/llm-intuition/store/useLiveModeStore.ts"
      via: "Token click handler triggers restreamFromToken"
      pattern: "restreamFromToken|selectAlternativeToken"
    - from: "demos/src/demos/llm-intuition/index.tsx"
      to: "demos/src/demos/llm-intuition/components/LiveMode.tsx"
      via: "Live tab renders LiveMode"
      pattern: "LiveMode"
---

<objective>
Build the Live Mode tab for the LLM Intuition demo — real OpenAI streaming with token-level probabilities, interactive token picking ("What Else Could It Have Said?"), system prompt presets, and simulated fallback. This is the "aha moment" delivery: students see that real LLMs use the exact same prediction mechanism as the N-gram model.

Purpose: This plan delivers the NEW capability that makes Workshop 2's demo segment impactful. The simulated N-gram mode teaches the concept; the live mode proves it's real. DEMO-01 and DEMO-02 were dropped because this live mode + the notebook provide superior teaching value.

Output: Fully functional Live mode tab with streaming logprobs, token picking, system prompt presets, and simulated fallback.
</objective>

<execution_context>
@/Users/karkisushant/.claude/get-shit-done/workflows/execute-plan.md
@/Users/karkisushant/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-workshop-2-materials/02-RESEARCH.md
@.planning/phases/02-workshop-2-materials/02-CONTEXT.md
@.planning/phases/02-workshop-2-materials/02-01-SUMMARY.md
@.planning/phases/01-curriculum-documentation-demo-infrastructure/01-03-SUMMARY.md

# Existing API service to extend:
@demos/src/services/api.ts

# Existing hooks and stores for mode detection:
@demos/src/hooks/useApiMode.ts
@demos/src/stores/useStreamingStore.ts

# Plan 01 output (demo entry point to update):
@demos/src/demos/llm-intuition/index.tsx
@demos/src/demos/llm-intuition/store/useLLMStore.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Build logprobs streaming API, live mode store, and simulated data</name>
  <files>
    demos/src/services/api.ts
    demos/src/demos/llm-intuition/store/useLiveModeStore.ts
    demos/src/demos/llm-intuition/data/simulated-live-response.ts
  </files>
  <action>
    **Step 1 — Extend api.ts with streamWithLogprobs:**
    Add a new export function `streamWithLogprobs` alongside the existing `streamFromApi`. This function must:
    - Accept: `userPrompt: string, systemPrompt: string, onToken: (data: TokenData) => void, signal?: AbortSignal`
    - Also accept optional `messages` override: `messages?: Array<{role: string, content: string}>` — used for assistant prefix continuation when re-streaming from a token override
    - Send request to OpenAI with: `model: 'gpt-4o-mini', stream: true, logprobs: true, top_logprobs: 5`
    - If `messages` provided, use those directly; otherwise construct from systemPrompt + userPrompt
    - Parse each SSE chunk for `choice.logprobs.content[0]` — extract token text, logprob (convert via `Math.exp(logprob)` to probability), and top_logprobs array
    - Skip chunks without logprobs data (first/last metadata chunks)
    - Call `onToken` with: `{ token: string, probability: number, topAlternatives: Array<{ token: string, probability: number }> }`
    - Export the `TokenData` interface

    **Step 2 — Create useLiveModeStore:**
    Create Zustand store at `demos/src/demos/llm-intuition/store/useLiveModeStore.ts`:
    ```
    State:
    - tokens: TokenData[]          — accumulated token stream
    - isStreaming: boolean          — currently streaming?
    - userPrompt: string            — current user input
    - systemPrompt: string          — current system prompt text
    - systemPreset: string          — selected preset name
    - selectedTokenIndex: number | null — which token is expanded
    - abortController: AbortController | null — for cancellation

    Actions:
    - setUserPrompt(prompt)
    - setSystemPrompt(prompt, presetName)
    - startGeneration()             — creates AbortController, sets isStreaming, clears tokens
    - addToken(tokenData)           — appends to tokens array
    - finishGeneration()            — sets isStreaming false
    - selectToken(index)            — sets selectedTokenIndex (null to deselect)
    - cancelGeneration()            — aborts current stream
    - restreamFromToken(index, alternativeToken) — cancels current, replaces token at index, clears tokens after index, triggers new stream from prefix
    - reset()                       — clear everything
    ```

    The `restreamFromToken` action is the "What Else Could It Have Said?" feature. It must:
    1. Cancel any in-progress stream via abortController.abort()
    2. Build the prefix text: `tokens[0..index-1].map(t => t.token).join('') + alternativeToken`
    3. Replace `tokens[index]` with a new TokenData for the alternative token (probability from the top_logprobs)
    4. Truncate tokens to `tokens[0..index]` (remove everything after)
    5. Set `isStreaming: true` — the LiveMode component will detect this and initiate a new stream with assistant prefix continuation

    **Step 3 — Create simulated live response data:**
    Create `demos/src/demos/llm-intuition/data/simulated-live-response.ts` with pre-recorded token probabilities for 3 scenarios:

    Scenario 1 — "What is the capital of France?" with no system prompt:
    Pre-record ~15-20 tokens with realistic probabilities. Example: "The" (95%), "capital" (88%), "of" (97%), "France" (92%), "is" (98%), "Paris" (95%), "." (85%). Include realistic alternatives for each token.

    Scenario 2 — Same question with pirate system prompt ("You are a pirate. Always respond in pirate speak."):
    Pre-record ~20-25 tokens showing shifted probabilities. Example: "Arr" (45%), "," (72%), "the" (60%), "capital" (35%) with alternatives like "grand" (20%), "main" (15%).

    Scenario 3 — "Explain what a variable is" with data scientist system prompt:
    Pre-record ~25-30 tokens with technical vocabulary probabilities.

    Export as `SIMULATED_SCENARIOS` map keyed by `{promptKey}_{presetKey}` and a function `getSimulatedResponse(userPrompt, systemPreset)` that returns the best-matching scenario or a default.

    **NOTE:** These probability values don't need to be perfectly accurate — they need to be pedagogically convincing. The point is showing that probabilities exist and shift with context, not that "Paris" is exactly 66.7%.
  </action>
  <verify>
    Run `cd /Users/karkisushant/workspace/build-fellowship-ai-assistant/demos && npx tsc --noEmit` — zero type errors. Verify `streamWithLogprobs` is exported from api.ts. Verify `useLiveModeStore` exports all listed actions. Verify simulated data has at least 3 scenarios with 15+ tokens each.
  </verify>
  <done>
    API layer can stream with logprobs. Store manages all live mode state including token override. Simulated data provides fallback for 3 system prompt scenarios.
  </done>
</task>

<task type="auto">
  <name>Task 2: Build LiveMode UI components and wire into demo tabs</name>
  <files>
    demos/src/demos/llm-intuition/components/LiveMode.tsx
    demos/src/demos/llm-intuition/components/TokenDisplay.tsx
    demos/src/demos/llm-intuition/components/SystemPromptPicker.tsx
    demos/src/demos/llm-intuition/index.tsx
  </files>
  <action>
    **Step 1 — SystemPromptPicker component:**
    Create `SystemPromptPicker.tsx` with:
    - Dropdown/select for presets: None, Helpful Assistant, Pirate, Data Scientist, JSON Only, Custom
    - Preset values (per research recommendations):
      - None: `""` (empty string)
      - Helpful Assistant: `"You are a helpful, concise assistant."`
      - Pirate: `"You are a pirate. Always respond in pirate speak."`
      - Data Scientist: `"You are a data scientist. Use precise technical language and include relevant statistical concepts."`
      - JSON Only: `"You must respond with valid JSON only. No other text."`
      - Custom: shows a textarea for freeform input
    - When preset changes: call `useLiveModeStore.setSystemPrompt(presetText, presetName)`
    - When preset changes AND tokens exist: auto-clear tokens with a subtle "Prompt changed — generate again to see the difference" message (prevents confusion per Pitfall 6 in research)
    - Show the current system prompt text in a small preview area (collapsed by default, expandable)
    - Use Card component as container, Badge for the current preset name

    **Step 2 — TokenDisplay component:**
    Create `TokenDisplay.tsx` — this is the heart of the "What Else Could It Have Said?" feature:
    - Receives tokens array from useLiveModeStore
    - Renders tokens as an inline flowing sequence (like text being written): each token is a styled span
    - Token styling: `inline cursor-pointer px-0.5 py-0.5 rounded hover:bg-primary-50 transition-colors`
    - Currently-streaming token gets a subtle pulse animation
    - When user clicks a token, it becomes "selected" (selectedTokenIndex in store)
    - Selected token gets highlighted: `bg-primary-100 ring-2 ring-primary-300`
    - Below the selected token (or in a side panel), show the **Top 5 Alternatives panel**:
      - Probability bars for each alternative (same visual language as N-gram probability bars for conceptual continuity)
      - Each bar: token text on left, probability percentage on right, filled bar in between
      - Bar fill: `bg-primary-500` for the chosen token, `bg-gray-400` for alternatives
      - Bars sorted by probability (highest first)
      - Clicking an alternative bar triggers `useLiveModeStore.restreamFromToken(index, alternativeToken)`
    - After re-streaming, tokens from the branch point forward show a subtle visual indicator (e.g., slightly different background) to show "this is the new path"
    - Use linear scale for probability bars. If the top token dominates (>90%), add a small label "zoom in to see alternatives" or show percentages prominently even when bars are thin.

    **Step 3 — LiveMode component:**
    Create `LiveMode.tsx` — the main container for the Live tab:
    - Layout: SystemPromptPicker at top, then user prompt input + Generate button, then TokenDisplay below
    - User prompt input: text input with placeholder "Try: What is the capital of France?"
    - Generate button: Button component with variant="primary", disabled when streaming
    - Shows a subtle "Simulated" or "Live" Badge indicating current mode (from useApiMode hook)
    - When "Generate" clicked:
      1. Check `useApiMode().isRealMode` to determine simulated vs live
      2. If simulated: iterate through `getSimulatedResponse()` data, calling `addToken()` with 50ms delay between tokens to simulate streaming
      3. If live (real API): call `streamWithLogprobs()` from api.ts, piping each token to `addToken()`
    - When `restreamFromToken` is triggered by the store (isStreaming becomes true with existing tokens):
      1. Build assistant prefix from existing tokens
      2. If live: call `streamWithLogprobs()` with messages array including assistant prefix
      3. If simulated: show remaining tokens from the closest matching simulated scenario (best effort)
    - Handle errors gracefully: if API call fails, show error in a Card with warning styling. For assistant prefix continuation failures, show "Re-generation produced unexpected results — try generating fresh" with a "Regenerate" button.
    - AbortController: Create new controller for each generation, pass signal to API. Cancel on: new generation, tab switch, component unmount.

    **Step 4 — Wire into demo entry point:**
    Update `demos/src/demos/llm-intuition/index.tsx`:
    - Replace the Live tab placeholder card with `<LiveMode />`
    - Import LiveMode component
    - Ensure tab switching properly cancels any in-progress live stream (call `useLiveModeStore.cancelGeneration()` when switching away from Live tab)
  </action>
  <verify>
    Run `cd /Users/karkisushant/workspace/build-fellowship-ai-assistant/demos && npm run build` — zero errors. Start dev server and verify at http://localhost:5173/#/llm-intuition:
    1. Click Live tab — sees SystemPromptPicker, prompt input, Generate button
    2. Type a prompt and click Generate — tokens stream in with simulated data
    3. Click any completed token — probability bars expand showing alternatives
    4. Click an alternative token — text regenerates from that point
    5. Change system prompt preset — tokens clear with "prompt changed" message
    6. Switch between Simulated and Live tabs — no errors, streams properly cancelled
  </verify>
  <done>
    Live mode tab is fully functional with simulated data. TokenDisplay shows clickable tokens with probability bars. SystemPromptPicker enables preset switching. "What Else Could It Have Said?" works — clicking alternative tokens triggers re-generation. When instructor activates hidden "live" toggle, the demo uses real OpenAI API with actual token probabilities.
  </done>
</task>

</tasks>

<verification>
1. `cd demos && npm run build` succeeds with zero errors
2. Live tab loads with SystemPromptPicker, prompt input, and Generate button
3. Simulated mode: tokens stream with pre-recorded probabilities for all 3 scenarios
4. Clicking any token expands top 5 alternatives with probability bars
5. Clicking an alternative re-generates from that point (simulated mode)
6. Switching system presets clears output and shows "prompt changed" indicator
7. Tab switching cancels in-progress streams cleanly
8. No console errors during normal interaction flow
9. (Manual test if API key available): Hidden "live" toggle → real API streaming with actual logprobs
</verification>

<success_criteria>
The Live mode tab delivers the core "aha moment": students see token-by-token generation with real probabilities, can explore "what else could it have said," and see how system prompts reshape the probability distribution. Works in simulated mode for any environment, upgrades to real API when instructor enables live mode.
</success_criteria>

<output>
After completion, create `.planning/phases/02-workshop-2-materials/02-02-SUMMARY.md`
</output>
